{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting and Data Augmentation Summary\n",
    "\n",
    "## 1. Data Splitting Process\n",
    "   - Split each dataset into train, validation, and test sets using multiple ratios: 60/20/20, 70/15/15, and 80/10/10.\n",
    "   - The splitting was performed on all three individual datasets as well as the combined dataset.\n",
    "   - Ensured consistent directory structure and proper allocation across splits.\n",
    "   - **Details**:\n",
    "     - Utilized a shuffling mechanism to ensure randomness in split allocation.\n",
    "     - **Directory Structure**:\n",
    "       - Created `train`, `val`, and `test` directories for each class in each dataset split.\n",
    "   - **Outcome**:\n",
    "     - Generated separate training, validation, and testing datasets for different split ratios.\n",
    "     - Provided detailed statistics for per-class and overall splits.\n",
    "\n",
    "## 2. Data Augmentation Process\n",
    "   - Applied data augmentations to the pre-split datasets across all splits (60/20/20, 70/15/15, and 80/10/10).\n",
    "   - Augmentation was performed independently for train, validation, and test splits.\n",
    "   - **Augmentation Techniques**:\n",
    "     - **Transformations Used**:\n",
    "       - `RandomResizedCrop`: Randomly resized crops of images.\n",
    "       - `RandomRotation`: Applied random rotations.\n",
    "       - `RandomHorizontalFlip`: Randomly flipped images horizontally.\n",
    "       - `ColorJitter`: Adjusted brightness, contrast, saturation, and hue.\n",
    "       - `RandomAffine`: Applied random affine transformations.\n",
    "       - `RandomErasing`: Performed random erasing for data augmentation.\n",
    "     - **Augmentation Count**:\n",
    "       - Generated 10 augmented images per original image.\n",
    "   - **Outcome**:\n",
    "     - Created enhanced datasets with multiple augmentations per original image across different splits and combinations.\n",
    "     - Detailed statistics and summaries provided for augmented data.\n",
    "\n",
    "## 3. Tools and Libraries Utilized\n",
    "   - **Data Splitting**: Utilized Python's `os`, `shutil`, and `random` libraries for file handling and directory management.\n",
    "   - **Image Augmentation**: Used `PIL` for image handling and `torchvision.transforms` for augmentations.\n",
    "   - **Progress Monitoring**: Employed `tqdm` for tracking file operations and augmentation processes.\n",
    "\n",
    "## 4. Final Results\n",
    "   - Delivered train, validation, and test splits with consistent class distribution across various split ratios.\n",
    "   - Generated augmented datasets with comprehensive transformations to increase data diversity.\n",
    "   - Processed all three individual datasets as well as the combined dataset.\n",
    "   - Provided detailed documentation and summaries, including per-class statistics and overall dataset statistics for each stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /scratch/movi/dmp/lib/python3.9/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (1.8.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: zipp>=3.20 in /scratch/movi/dmp/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.20.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /scratch/movi/dmp/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec dmp in /home/movi/.local/share/jupyter/kernels/dmp\n"
     ]
    }
   ],
   "source": [
    "# # Create the virtual environment named 'dmp'\n",
    "!python3 -m venv /scratch/movi/dmp\n",
    "# Install ipykernel inside the 'dmp' environment\n",
    "!/scratch/movi/dmp/bin/pip install ipykernel\n",
    "# Add 'dmp' as a kernel for Jupyter Notebook\n",
    "!/scratch/movi/dmp/bin/python -m ipykernel install --user --name=dmp --display-name \"Python (dmp)\"\n",
    "# # Upgrade pip in the 'dmp' environment\n",
    "# !/scratch/movi/dmp/bin/python3 -m pip install --upgrade pip\n",
    "# # Install necessary packages (NumPy, PyTorch, etc.) inside 'dmp'\n",
    "# !/scratch/movi/dmp/bin/pip install numpy torch torchvision torchaudio pandas matplotlib scikit-learn\n",
    "\n",
    "\n",
    "# !pip uninstall -y tensorflow\n",
    "# !pip install numpy==1.21.4 scikit-learn==1.0.2\n",
    "# import tensorflow as tf\n",
    "# print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.9 (main, Mar 25 2022, 16:08:31) \n",
      "[GCC 10.3.0]\n",
      "NumPy Version: 1.21.4\n",
      "PyTorch Version: 1.12.1+cu113\n",
      "CUDA is available. PyTorch is using GPU.\n",
      "\n",
      "Number of GPUs available: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-80GB MIG 3g.40gb\n",
      "  Total Memory: 39.25 GB\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Reserved (Cached): 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Prints the installed versions of Python, NumPy, and PyTorch libraries\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "\n",
    "\n",
    "# Function to check GPU availability and display memory statistics using PyTorch's CUDA interface\n",
    "\n",
    "def check_gpu_status():\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA is available. PyTorch is using GPU.\\n\")\n",
    "\n",
    "        # Get the number of available GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "        # Loop through each GPU and display its details\n",
    "        for gpu_id in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)  # In GB\n",
    "            gpu_memory_cached = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)      # In GB\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024 ** 3)  # In GB\n",
    "\n",
    "            print(f\"\\nGPU {gpu_id}: {gpu_name}\")\n",
    "            print(f\"  Total Memory: {gpu_memory_total:.2f} GB\")\n",
    "            print(f\"  Memory Allocated: {gpu_memory_allocated:.2f} GB\")\n",
    "            print(f\"  Memory Reserved (Cached): {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. PyTorch is using the CPU.\")\n",
    "\n",
    "# Run the GPU status check\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splits dataset into train/validation/test sets with specified ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 24 train, 5 val, 6 test (Total: 35)\n",
      "10: 110 train, 23 val, 25 test (Total: 158)\n",
      "100: 110 train, 23 val, 25 test (Total: 158)\n",
      "1000: 60 train, 13 val, 14 test (Total: 87)\n",
      "2: 111 train, 23 val, 25 test (Total: 159)\n",
      "20: 102 train, 21 val, 23 test (Total: 146)\n",
      "200: 0 train, 0 val, 0 test (Total: 0)\n",
      "5: 104 train, 22 val, 23 test (Total: 149)\n",
      "50: 98 train, 21 val, 21 test (Total: 140)\n",
      "500: 77 train, 16 val, 17 test (Total: 110)\n",
      "\n",
      "Overall Dataset Summary:\n",
      "Total Images: 1142\n",
      "Train Ratio: 0.7, Validation Ratio: 0.15, Test Ratio: 0.15\n",
      "\n",
      "Detailed Split Summary:\n",
      "1 - Total: 35, Train: 24, Val: 5, Test: 6\n",
      "10 - Total: 158, Train: 110, Val: 23, Test: 25\n",
      "100 - Total: 158, Train: 110, Val: 23, Test: 25\n",
      "1000 - Total: 87, Train: 60, Val: 13, Test: 14\n",
      "2 - Total: 159, Train: 111, Val: 23, Test: 25\n",
      "20 - Total: 146, Train: 102, Val: 21, Test: 23\n",
      "200 - Total: 0, Train: 0, Val: 0, Test: 0\n",
      "5 - Total: 149, Train: 104, Val: 22, Test: 23\n",
      "50 - Total: 140, Train: 98, Val: 21, Test: 21\n",
      "500 - Total: 110, Train: 77, Val: 16, Test: 17\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi\n",
    "# Code to split dataset into train, validation, and test sets, with detailed analysis and logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset = '/scratch/movi/dm_project/data/dataset1_unique'  # Replace with the path to your original dataset\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_70/dataset1_split'        # Base directory to store train/val/test splits\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "\n",
    "def create_dir_structure(base_dir, class_names):\n",
    "    \"\"\"Create train, val, and test directories for each class.\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "def analyze_and_split_dataset(original_dataset, split_base_dir):\n",
    "    \"\"\"Analyze dataset and split into train, val, and test sets.\"\"\"\n",
    "    class_names = sorted(os.listdir(original_dataset))  # Get class names in alphabetical order\n",
    "    create_dir_structure(split_base_dir, class_names)   # Create the necessary directory structure\n",
    "\n",
    "    total_images = 0  # Track the total number of images across all classes\n",
    "    split_summary = {}  # Dictionary to store per-class split details\n",
    "\n",
    "    # Loop through each class folder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(original_dataset, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):  # Ensure it's a folder\n",
    "            # List all images in the class folder\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            random.shuffle(image_files)  # Shuffle images to ensure randomness\n",
    "\n",
    "            # Calculate split indices\n",
    "            total_images_in_class = len(image_files)\n",
    "            train_end = int(total_images_in_class * TRAIN_RATIO)\n",
    "            val_end = train_end + int(total_images_in_class * VAL_RATIO)\n",
    "\n",
    "            # Split the image files into train, val, and test sets\n",
    "            train_files = image_files[:train_end]\n",
    "            val_files = image_files[train_end:val_end]\n",
    "            test_files = image_files[val_end:]\n",
    "\n",
    "            # Copy files to the respective split directories\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'train', class_name, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'val', class_name, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'test', class_name, file))\n",
    "\n",
    "            # Store the split summary for this class\n",
    "            split_summary[class_name] = {\n",
    "                'Total': total_images_in_class,\n",
    "                'Train': len(train_files),\n",
    "                'Validation': len(val_files),\n",
    "                'Test': len(test_files)\n",
    "            }\n",
    "\n",
    "            # Update total image count\n",
    "            total_images += total_images_in_class\n",
    "\n",
    "            # Print per-class summary\n",
    "            print(f\"{class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test (Total: {total_images_in_class})\")\n",
    "\n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Dataset Summary:\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Train Ratio: {TRAIN_RATIO}, Validation Ratio: {VAL_RATIO}, Test Ratio: {TEST_RATIO}\\n\")\n",
    "\n",
    "    # Print detailed split summary for all classes\n",
    "    print(\"Detailed Split Summary:\")\n",
    "    for class_name, counts in split_summary.items():\n",
    "        print(f\"{class_name} - Total: {counts['Total']}, Train: {counts['Train']}, Val: {counts['Validation']}, Test: {counts['Test']}\")\n",
    "\n",
    "    return split_summary\n",
    "\n",
    "# Run the split function and store the summary\n",
    "dataset_summary = analyze_and_split_dataset(original_dataset, split_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Augmentation Process ===\n",
      "\n",
      "Processing TRAIN split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 24 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 24/24 [00:00<00:00, 221.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 24/24 [00:02<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 110 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 110/110 [00:00<00:00, 219.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 110/110 [00:12<00:00,  8.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 110 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 110/110 [00:00<00:00, 227.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 110/110 [00:13<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 60 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 60/60 [00:00<00:00, 181.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 60/60 [00:08<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 111 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 111/111 [00:00<00:00, 115.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 111/111 [00:11<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 102 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 102/102 [00:00<00:00, 248.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 102/102 [00:10<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 104 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 104/104 [00:00<00:00, 209.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 104/104 [00:23<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 98 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 98/98 [00:00<00:00, 221.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 98/98 [00:23<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 77 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 77/77 [00:00<00:00, 216.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 77/77 [00:12<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VAL split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 5 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 5/5 [00:00<00:00, 196.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 5/5 [00:00<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 230.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 203.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 212.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:02<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 188.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 21 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 21/21 [00:00<00:00, 225.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 21/21 [00:02<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 22 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 22/22 [00:00<00:00, 236.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 22/22 [00:03<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 21 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 21/21 [00:00<00:00, 217.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 21/21 [00:02<00:00,  8.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 16 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 16/16 [00:00<00:00, 200.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 16/16 [00:01<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TEST split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 6 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 6/6 [00:00<00:00, 250.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 6/6 [00:00<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 230.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:02<00:00, 10.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 191.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:06<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 14 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 14/14 [00:00<00:00, 231.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 14/14 [00:01<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 239.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:02<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 216.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 125.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 21 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 21/21 [00:00<00:00, 194.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 21/21 [00:02<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 17 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 17/17 [00:00<00:00, 214.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 17/17 [00:02<00:00,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Summary ===\n",
      "\n",
      "Per Split Statistics:\n",
      "\n",
      "TRAIN Split:\n",
      "Class 1:\n",
      "  Original: 24\n",
      "  Augmented: 240\n",
      "  Total: 264\n",
      "Class 10:\n",
      "  Original: 110\n",
      "  Augmented: 1100\n",
      "  Total: 1210\n",
      "Class 100:\n",
      "  Original: 110\n",
      "  Augmented: 1100\n",
      "  Total: 1210\n",
      "Class 1000:\n",
      "  Original: 60\n",
      "  Augmented: 600\n",
      "  Total: 660\n",
      "Class 2:\n",
      "  Original: 111\n",
      "  Augmented: 1110\n",
      "  Total: 1221\n",
      "Class 20:\n",
      "  Original: 102\n",
      "  Augmented: 1020\n",
      "  Total: 1122\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 104\n",
      "  Augmented: 1040\n",
      "  Total: 1144\n",
      "Class 50:\n",
      "  Original: 98\n",
      "  Augmented: 980\n",
      "  Total: 1078\n",
      "Class 500:\n",
      "  Original: 77\n",
      "  Augmented: 770\n",
      "  Total: 847\n",
      "\n",
      "TRAIN Split Totals:\n",
      "  Original Images: 796\n",
      "  Augmented Images: 7960\n",
      "  Total Images: 8756\n",
      "--------------------------------------------------\n",
      "\n",
      "VAL Split:\n",
      "Class 1:\n",
      "  Original: 5\n",
      "  Augmented: 50\n",
      "  Total: 55\n",
      "Class 10:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 100:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 1000:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 2:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 20:\n",
      "  Original: 21\n",
      "  Augmented: 210\n",
      "  Total: 231\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 22\n",
      "  Augmented: 220\n",
      "  Total: 242\n",
      "Class 50:\n",
      "  Original: 21\n",
      "  Augmented: 210\n",
      "  Total: 231\n",
      "Class 500:\n",
      "  Original: 16\n",
      "  Augmented: 160\n",
      "  Total: 176\n",
      "\n",
      "VAL Split Totals:\n",
      "  Original Images: 167\n",
      "  Augmented Images: 1670\n",
      "  Total Images: 1837\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST Split:\n",
      "Class 1:\n",
      "  Original: 6\n",
      "  Augmented: 60\n",
      "  Total: 66\n",
      "Class 10:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 100:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 1000:\n",
      "  Original: 14\n",
      "  Augmented: 140\n",
      "  Total: 154\n",
      "Class 2:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 20:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 50:\n",
      "  Original: 21\n",
      "  Augmented: 210\n",
      "  Total: 231\n",
      "Class 500:\n",
      "  Original: 17\n",
      "  Augmented: 170\n",
      "  Total: 187\n",
      "\n",
      "TEST Split Totals:\n",
      "  Original Images: 179\n",
      "  Augmented Images: 1790\n",
      "  Total Images: 1969\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Dataset Statistics:\n",
      "Total Original Images: 1142\n",
      "Total Augmented Images: 11420\n",
      "Total Images: 12562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-03\n",
    "# Code to apply augmentations to pre-split dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths - update these paths\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_70/dataset1_split'  # Your already split dataset\n",
    "augmented_data_dir = '/scratch/movi/dm_project/data/split_70/dataset1_aug'  # Where to save augmented data\n",
    "\n",
    "# Number of augmentations per image\n",
    "NUM_AUGMENTATIONS = 10\n",
    "\n",
    "# Define augmentation transformations\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "def apply_augmentations():\n",
    "    \"\"\"Apply augmentations to each split of the pre-split dataset.\"\"\"\n",
    "    print(\"\\n=== Starting Augmentation Process ===\")\n",
    "    \n",
    "    # Create destination directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(augmented_data_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for class_name in os.listdir(os.path.join(split_base_dir, split)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "    # Stats dictionary\n",
    "    stats = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "    # Process each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split:\")\n",
    "        split_source = os.path.join(split_base_dir, split)\n",
    "        split_dest = os.path.join(augmented_data_dir, split)\n",
    "        \n",
    "        # Process each class\n",
    "        for class_name in sorted(os.listdir(split_source)):\n",
    "            class_source = os.path.join(split_source, class_name)\n",
    "            class_dest = os.path.join(split_dest, class_name)\n",
    "            \n",
    "            if os.path.isdir(class_source):\n",
    "                # Get list of original images\n",
    "                original_files = [f for f in os.listdir(class_source) \n",
    "                                if os.path.isfile(os.path.join(class_source, f))]\n",
    "                \n",
    "                print(f\"\\nProcessing class {class_name}:\")\n",
    "                print(f\"Found {len(original_files)} original images\")\n",
    "                \n",
    "                # First copy original files\n",
    "                for file in tqdm(original_files, desc=\"Copying originals\"):\n",
    "                    shutil.copy2(os.path.join(class_source, file),\n",
    "                               os.path.join(class_dest, file))\n",
    "                \n",
    "                # Then create augmented versions\n",
    "                print(\"Generating augmented images...\")\n",
    "                for file in tqdm(original_files, desc=\"Generating augmentations\"):\n",
    "                    img_path = os.path.join(class_source, file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            # Convert to RGB if needed\n",
    "                            if img.mode != 'RGB':\n",
    "                                img = img.convert('RGB')\n",
    "                            \n",
    "                            img_tensor = transforms.ToTensor()(img)\n",
    "                            \n",
    "                            # Generate augmentations\n",
    "                            for i in range(NUM_AUGMENTATIONS):\n",
    "                                try:\n",
    "                                    augmented_tensor = augmentation_transforms(img_tensor)\n",
    "                                    augmented_img = transforms.ToPILImage()(augmented_tensor)\n",
    "                                    \n",
    "                                    # Save augmented image\n",
    "                                    base_name = os.path.splitext(file)[0]\n",
    "                                    aug_name = f\"{base_name}_aug_{i+1}.jpg\"\n",
    "                                    augmented_img.save(os.path.join(class_dest, aug_name))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error generating augmentation {i+1} for {file}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file}: {str(e)}\")\n",
    "                \n",
    "                # Update stats\n",
    "                total_augmented = len(original_files) * NUM_AUGMENTATIONS\n",
    "                stats[split][class_name] = {\n",
    "                    'original': len(original_files),\n",
    "                    'augmented': total_augmented,\n",
    "                    'total': len(original_files) + total_augmented\n",
    "                }\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n=== Augmentation Summary ===\")\n",
    "    print(\"\\nPer Split Statistics:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        split_total_orig = 0\n",
    "        split_total_aug = 0\n",
    "        \n",
    "        for class_name, counts in sorted(stats[split].items()):\n",
    "            print(f\"Class {class_name}:\")\n",
    "            print(f\"  Original: {counts['original']}\")\n",
    "            print(f\"  Augmented: {counts['augmented']}\")\n",
    "            print(f\"  Total: {counts['total']}\")\n",
    "            split_total_orig += counts['original']\n",
    "            split_total_aug += counts['augmented']\n",
    "        \n",
    "        print(f\"\\n{split.upper()} Split Totals:\")\n",
    "        print(f\"  Original Images: {split_total_orig}\")\n",
    "        print(f\"  Augmented Images: {split_total_aug}\")\n",
    "        print(f\"  Total Images: {split_total_orig + split_total_aug}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Overall totals\n",
    "    total_orig = sum(sum(c['original'] for c in s.values()) for s in stats.values())\n",
    "    total_aug = sum(sum(c['augmented'] for c in s.values()) for s in stats.values())\n",
    "    print(\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"Total Original Images: {total_orig}\")\n",
    "    print(f\"Total Augmented Images: {total_aug}\")\n",
    "    print(f\"Total Images: {total_orig + total_aug}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 28 train, 3 val, 5 test (Total: 36)\n",
      "10: 107 train, 13 val, 14 test (Total: 134)\n",
      "100: 111 train, 13 val, 15 test (Total: 139)\n",
      "1000: 60 train, 7 val, 9 test (Total: 76)\n",
      "2: 102 train, 12 val, 14 test (Total: 128)\n",
      "20: 104 train, 13 val, 13 test (Total: 130)\n",
      "200: 15 train, 1 val, 3 test (Total: 19)\n",
      "5: 96 train, 12 val, 13 test (Total: 121)\n",
      "50: 94 train, 11 val, 13 test (Total: 118)\n",
      "500: 72 train, 9 val, 10 test (Total: 91)\n",
      "\n",
      "Overall Dataset Summary:\n",
      "Total Images: 992\n",
      "Train Ratio: 0.8, Validation Ratio: 0.1, Test Ratio: 0.1\n",
      "\n",
      "Detailed Split Summary:\n",
      "1 - Total: 36, Train: 28, Val: 3, Test: 5\n",
      "10 - Total: 134, Train: 107, Val: 13, Test: 14\n",
      "100 - Total: 139, Train: 111, Val: 13, Test: 15\n",
      "1000 - Total: 76, Train: 60, Val: 7, Test: 9\n",
      "2 - Total: 128, Train: 102, Val: 12, Test: 14\n",
      "20 - Total: 130, Train: 104, Val: 13, Test: 13\n",
      "200 - Total: 19, Train: 15, Val: 1, Test: 3\n",
      "5 - Total: 121, Train: 96, Val: 12, Test: 13\n",
      "50 - Total: 118, Train: 94, Val: 11, Test: 13\n",
      "500 - Total: 91, Train: 72, Val: 9, Test: 10\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi\n",
    "# Code to split dataset into train, validation, and test sets, with detailed analysis and logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset = '/scratch/movi/dm_project/data/dataset2_unique'  # Replace with the path to your original dataset\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset2_split'        # Base directory to store train/val/test splits\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "def create_dir_structure(base_dir, class_names):\n",
    "    \"\"\"Create train, val, and test directories for each class.\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "def analyze_and_split_dataset(original_dataset, split_base_dir):\n",
    "    \"\"\"Analyze dataset and split into train, val, and test sets.\"\"\"\n",
    "    class_names = sorted(os.listdir(original_dataset))  # Get class names in alphabetical order\n",
    "    create_dir_structure(split_base_dir, class_names)   # Create the necessary directory structure\n",
    "\n",
    "    total_images = 0  # Track the total number of images across all classes\n",
    "    split_summary = {}  # Dictionary to store per-class split details\n",
    "\n",
    "    # Loop through each class folder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(original_dataset, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):  # Ensure it's a folder\n",
    "            # List all images in the class folder\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            random.shuffle(image_files)  # Shuffle images to ensure randomness\n",
    "\n",
    "            # Calculate split indices\n",
    "            total_images_in_class = len(image_files)\n",
    "            train_end = int(total_images_in_class * TRAIN_RATIO)\n",
    "            val_end = train_end + int(total_images_in_class * VAL_RATIO)\n",
    "\n",
    "            # Split the image files into train, val, and test sets\n",
    "            train_files = image_files[:train_end]\n",
    "            val_files = image_files[train_end:val_end]\n",
    "            test_files = image_files[val_end:]\n",
    "\n",
    "            # Copy files to the respective split directories\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'train', class_name, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'val', class_name, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'test', class_name, file))\n",
    "\n",
    "            # Store the split summary for this class\n",
    "            split_summary[class_name] = {\n",
    "                'Total': total_images_in_class,\n",
    "                'Train': len(train_files),\n",
    "                'Validation': len(val_files),\n",
    "                'Test': len(test_files)\n",
    "            }\n",
    "\n",
    "            # Update total image count\n",
    "            total_images += total_images_in_class\n",
    "\n",
    "            # Print per-class summary\n",
    "            print(f\"{class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test (Total: {total_images_in_class})\")\n",
    "\n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Dataset Summary:\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Train Ratio: {TRAIN_RATIO}, Validation Ratio: {VAL_RATIO}, Test Ratio: {TEST_RATIO}\\n\")\n",
    "\n",
    "    # Print detailed split summary for all classes\n",
    "    print(\"Detailed Split Summary:\")\n",
    "    for class_name, counts in split_summary.items():\n",
    "        print(f\"{class_name} - Total: {counts['Total']}, Train: {counts['Train']}, Val: {counts['Validation']}, Test: {counts['Test']}\")\n",
    "\n",
    "    return split_summary\n",
    "\n",
    "# Run the split function and store the summary\n",
    "dataset_summary = analyze_and_split_dataset(original_dataset, split_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Augmentation Process ===\n",
      "\n",
      "Processing TRAIN split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 28 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 28/28 [00:00<00:00, 199.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 28/28 [00:05<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 107 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 107/107 [00:00<00:00, 206.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 107/107 [00:15<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 111 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 111/111 [00:00<00:00, 206.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 111/111 [00:40<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 60 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 60/60 [00:00<00:00, 236.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 60/60 [00:07<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 102 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 102/102 [00:00<00:00, 171.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 102/102 [00:17<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 104 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 104/104 [00:00<00:00, 246.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 104/104 [00:10<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 15 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 15/15 [00:00<00:00, 129.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 15/15 [00:03<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 96 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 96/96 [00:00<00:00, 194.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 96/96 [00:10<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 94 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 94/94 [00:00<00:00, 191.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 94/94 [00:22<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 72 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 72/72 [00:00<00:00, 198.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 72/72 [00:07<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VAL split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 3 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 3/3 [00:00<00:00, 189.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 3/3 [00:00<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 210.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:01<00:00, 10.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 165.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:01<00:00, 10.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 7 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 7/7 [00:00<00:00, 176.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 7/7 [00:00<00:00, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 12 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 12/12 [00:00<00:00, 196.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 12/12 [00:01<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 210.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:17<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 1 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 1/1 [00:00<00:00, 193.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 1/1 [00:00<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 12 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 12/12 [00:00<00:00, 158.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 12/12 [00:04<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 11 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 11/11 [00:00<00:00, 228.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 11/11 [00:01<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 9 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 9/9 [00:00<00:00, 207.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 9/9 [00:05<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TEST split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 5 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 5/5 [00:00<00:00, 205.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 5/5 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 14 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 14/14 [00:00<00:00, 40.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 14/14 [00:01<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 15 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 15/15 [00:00<00:00, 208.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 15/15 [00:01<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 9 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 9/9 [00:00<00:00, 219.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 9/9 [00:00<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 14 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 14/14 [00:00<00:00, 38.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 14/14 [00:09<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 161.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:01<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 3 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 3/3 [00:00<00:00, 195.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 3/3 [00:00<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 219.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:01<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 13 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 13/13 [00:00<00:00, 208.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 13/13 [00:01<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 10 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 10/10 [00:00<00:00, 218.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 10/10 [00:00<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Summary ===\n",
      "\n",
      "Per Split Statistics:\n",
      "\n",
      "TRAIN Split:\n",
      "Class 1:\n",
      "  Original: 28\n",
      "  Augmented: 280\n",
      "  Total: 308\n",
      "Class 10:\n",
      "  Original: 107\n",
      "  Augmented: 1070\n",
      "  Total: 1177\n",
      "Class 100:\n",
      "  Original: 111\n",
      "  Augmented: 1110\n",
      "  Total: 1221\n",
      "Class 1000:\n",
      "  Original: 60\n",
      "  Augmented: 600\n",
      "  Total: 660\n",
      "Class 2:\n",
      "  Original: 102\n",
      "  Augmented: 1020\n",
      "  Total: 1122\n",
      "Class 20:\n",
      "  Original: 104\n",
      "  Augmented: 1040\n",
      "  Total: 1144\n",
      "Class 200:\n",
      "  Original: 15\n",
      "  Augmented: 150\n",
      "  Total: 165\n",
      "Class 5:\n",
      "  Original: 96\n",
      "  Augmented: 960\n",
      "  Total: 1056\n",
      "Class 50:\n",
      "  Original: 94\n",
      "  Augmented: 940\n",
      "  Total: 1034\n",
      "Class 500:\n",
      "  Original: 72\n",
      "  Augmented: 720\n",
      "  Total: 792\n",
      "\n",
      "TRAIN Split Totals:\n",
      "  Original Images: 789\n",
      "  Augmented Images: 7890\n",
      "  Total Images: 8679\n",
      "--------------------------------------------------\n",
      "\n",
      "VAL Split:\n",
      "Class 1:\n",
      "  Original: 3\n",
      "  Augmented: 30\n",
      "  Total: 33\n",
      "Class 10:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 100:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 1000:\n",
      "  Original: 7\n",
      "  Augmented: 70\n",
      "  Total: 77\n",
      "Class 2:\n",
      "  Original: 12\n",
      "  Augmented: 120\n",
      "  Total: 132\n",
      "Class 20:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 200:\n",
      "  Original: 1\n",
      "  Augmented: 10\n",
      "  Total: 11\n",
      "Class 5:\n",
      "  Original: 12\n",
      "  Augmented: 120\n",
      "  Total: 132\n",
      "Class 50:\n",
      "  Original: 11\n",
      "  Augmented: 110\n",
      "  Total: 121\n",
      "Class 500:\n",
      "  Original: 9\n",
      "  Augmented: 90\n",
      "  Total: 99\n",
      "\n",
      "VAL Split Totals:\n",
      "  Original Images: 94\n",
      "  Augmented Images: 940\n",
      "  Total Images: 1034\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST Split:\n",
      "Class 1:\n",
      "  Original: 5\n",
      "  Augmented: 50\n",
      "  Total: 55\n",
      "Class 10:\n",
      "  Original: 14\n",
      "  Augmented: 140\n",
      "  Total: 154\n",
      "Class 100:\n",
      "  Original: 15\n",
      "  Augmented: 150\n",
      "  Total: 165\n",
      "Class 1000:\n",
      "  Original: 9\n",
      "  Augmented: 90\n",
      "  Total: 99\n",
      "Class 2:\n",
      "  Original: 14\n",
      "  Augmented: 140\n",
      "  Total: 154\n",
      "Class 20:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 200:\n",
      "  Original: 3\n",
      "  Augmented: 30\n",
      "  Total: 33\n",
      "Class 5:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 50:\n",
      "  Original: 13\n",
      "  Augmented: 130\n",
      "  Total: 143\n",
      "Class 500:\n",
      "  Original: 10\n",
      "  Augmented: 100\n",
      "  Total: 110\n",
      "\n",
      "TEST Split Totals:\n",
      "  Original Images: 109\n",
      "  Augmented Images: 1090\n",
      "  Total Images: 1199\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Dataset Statistics:\n",
      "Total Original Images: 992\n",
      "Total Augmented Images: 9920\n",
      "Total Images: 10912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-03\n",
    "# Code to apply augmentations to pre-split dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths - update these paths\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset2_split'  # Your already split dataset\n",
    "augmented_data_dir = '/scratch/movi/dm_project/data/split_80/dataset2_aug'  # Where to save augmented data\n",
    "\n",
    "# Number of augmentations per image\n",
    "NUM_AUGMENTATIONS = 10\n",
    "\n",
    "# Define augmentation transformations\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "def apply_augmentations():\n",
    "    \"\"\"Apply augmentations to each split of the pre-split dataset.\"\"\"\n",
    "    print(\"\\n=== Starting Augmentation Process ===\")\n",
    "    \n",
    "    # Create destination directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(augmented_data_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for class_name in os.listdir(os.path.join(split_base_dir, split)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "    # Stats dictionary\n",
    "    stats = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "    # Process each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split:\")\n",
    "        split_source = os.path.join(split_base_dir, split)\n",
    "        split_dest = os.path.join(augmented_data_dir, split)\n",
    "        \n",
    "        # Process each class\n",
    "        for class_name in sorted(os.listdir(split_source)):\n",
    "            class_source = os.path.join(split_source, class_name)\n",
    "            class_dest = os.path.join(split_dest, class_name)\n",
    "            \n",
    "            if os.path.isdir(class_source):\n",
    "                # Get list of original images\n",
    "                original_files = [f for f in os.listdir(class_source) \n",
    "                                if os.path.isfile(os.path.join(class_source, f))]\n",
    "                \n",
    "                print(f\"\\nProcessing class {class_name}:\")\n",
    "                print(f\"Found {len(original_files)} original images\")\n",
    "                \n",
    "                # First copy original files\n",
    "                for file in tqdm(original_files, desc=\"Copying originals\"):\n",
    "                    shutil.copy2(os.path.join(class_source, file),\n",
    "                               os.path.join(class_dest, file))\n",
    "                \n",
    "                # Then create augmented versions\n",
    "                print(\"Generating augmented images...\")\n",
    "                for file in tqdm(original_files, desc=\"Generating augmentations\"):\n",
    "                    img_path = os.path.join(class_source, file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            # Convert to RGB if needed\n",
    "                            if img.mode != 'RGB':\n",
    "                                img = img.convert('RGB')\n",
    "                            \n",
    "                            img_tensor = transforms.ToTensor()(img)\n",
    "                            \n",
    "                            # Generate augmentations\n",
    "                            for i in range(NUM_AUGMENTATIONS):\n",
    "                                try:\n",
    "                                    augmented_tensor = augmentation_transforms(img_tensor)\n",
    "                                    augmented_img = transforms.ToPILImage()(augmented_tensor)\n",
    "                                    \n",
    "                                    # Save augmented image\n",
    "                                    base_name = os.path.splitext(file)[0]\n",
    "                                    aug_name = f\"{base_name}_aug_{i+1}.jpg\"\n",
    "                                    augmented_img.save(os.path.join(class_dest, aug_name))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error generating augmentation {i+1} for {file}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file}: {str(e)}\")\n",
    "                \n",
    "                # Update stats\n",
    "                total_augmented = len(original_files) * NUM_AUGMENTATIONS\n",
    "                stats[split][class_name] = {\n",
    "                    'original': len(original_files),\n",
    "                    'augmented': total_augmented,\n",
    "                    'total': len(original_files) + total_augmented\n",
    "                }\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n=== Augmentation Summary ===\")\n",
    "    print(\"\\nPer Split Statistics:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        split_total_orig = 0\n",
    "        split_total_aug = 0\n",
    "        \n",
    "        for class_name, counts in sorted(stats[split].items()):\n",
    "            print(f\"Class {class_name}:\")\n",
    "            print(f\"  Original: {counts['original']}\")\n",
    "            print(f\"  Augmented: {counts['augmented']}\")\n",
    "            print(f\"  Total: {counts['total']}\")\n",
    "            split_total_orig += counts['original']\n",
    "            split_total_aug += counts['augmented']\n",
    "        \n",
    "        print(f\"\\n{split.upper()} Split Totals:\")\n",
    "        print(f\"  Original Images: {split_total_orig}\")\n",
    "        print(f\"  Augmented Images: {split_total_aug}\")\n",
    "        print(f\"  Total Images: {split_total_orig + split_total_aug}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Overall totals\n",
    "    total_orig = sum(sum(c['original'] for c in s.values()) for s in stats.values())\n",
    "    total_aug = sum(sum(c['augmented'] for c in s.values()) for s in stats.values())\n",
    "    print(\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"Total Original Images: {total_orig}\")\n",
    "    print(f\"Total Augmented Images: {total_aug}\")\n",
    "    print(f\"Total Images: {total_orig + total_aug}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0 train, 0 val, 0 test (Total: 0)\n",
      "10: 160 train, 20 val, 20 test (Total: 200)\n",
      "100: 68 train, 8 val, 9 test (Total: 85)\n",
      "1000: 131 train, 16 val, 17 test (Total: 164)\n",
      "2: 77 train, 9 val, 11 test (Total: 97)\n",
      "20: 173 train, 21 val, 23 test (Total: 217)\n",
      "200: 0 train, 0 val, 0 test (Total: 0)\n",
      "5: 124 train, 15 val, 16 test (Total: 155)\n",
      "50: 172 train, 21 val, 23 test (Total: 216)\n",
      "500: 149 train, 18 val, 20 test (Total: 187)\n",
      "\n",
      "Overall Dataset Summary:\n",
      "Total Images: 1321\n",
      "Train Ratio: 0.8, Validation Ratio: 0.1, Test Ratio: 0.1\n",
      "\n",
      "Detailed Split Summary:\n",
      "1 - Total: 0, Train: 0, Val: 0, Test: 0\n",
      "10 - Total: 200, Train: 160, Val: 20, Test: 20\n",
      "100 - Total: 85, Train: 68, Val: 8, Test: 9\n",
      "1000 - Total: 164, Train: 131, Val: 16, Test: 17\n",
      "2 - Total: 97, Train: 77, Val: 9, Test: 11\n",
      "20 - Total: 217, Train: 173, Val: 21, Test: 23\n",
      "200 - Total: 0, Train: 0, Val: 0, Test: 0\n",
      "5 - Total: 155, Train: 124, Val: 15, Test: 16\n",
      "50 - Total: 216, Train: 172, Val: 21, Test: 23\n",
      "500 - Total: 187, Train: 149, Val: 18, Test: 20\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi\n",
    "# Code to split dataset into train, validation, and test sets, with detailed analysis and logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset = '/scratch/movi/dm_project/data/dataset3_unique'  # Replace with the path to your original dataset\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset3_split'        # Base directory to store train/val/test splits\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "def create_dir_structure(base_dir, class_names):\n",
    "    \"\"\"Create train, val, and test directories for each class.\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "def analyze_and_split_dataset(original_dataset, split_base_dir):\n",
    "    \"\"\"Analyze dataset and split into train, val, and test sets.\"\"\"\n",
    "    class_names = sorted(os.listdir(original_dataset))  # Get class names in alphabetical order\n",
    "    create_dir_structure(split_base_dir, class_names)   # Create the necessary directory structure\n",
    "\n",
    "    total_images = 0  # Track the total number of images across all classes\n",
    "    split_summary = {}  # Dictionary to store per-class split details\n",
    "\n",
    "    # Loop through each class folder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(original_dataset, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):  # Ensure it's a folder\n",
    "            # List all images in the class folder\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            random.shuffle(image_files)  # Shuffle images to ensure randomness\n",
    "\n",
    "            # Calculate split indices\n",
    "            total_images_in_class = len(image_files)\n",
    "            train_end = int(total_images_in_class * TRAIN_RATIO)\n",
    "            val_end = train_end + int(total_images_in_class * VAL_RATIO)\n",
    "\n",
    "            # Split the image files into train, val, and test sets\n",
    "            train_files = image_files[:train_end]\n",
    "            val_files = image_files[train_end:val_end]\n",
    "            test_files = image_files[val_end:]\n",
    "\n",
    "            # Copy files to the respective split directories\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'train', class_name, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'val', class_name, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'test', class_name, file))\n",
    "\n",
    "            # Store the split summary for this class\n",
    "            split_summary[class_name] = {\n",
    "                'Total': total_images_in_class,\n",
    "                'Train': len(train_files),\n",
    "                'Validation': len(val_files),\n",
    "                'Test': len(test_files)\n",
    "            }\n",
    "\n",
    "            # Update total image count\n",
    "            total_images += total_images_in_class\n",
    "\n",
    "            # Print per-class summary\n",
    "            print(f\"{class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test (Total: {total_images_in_class})\")\n",
    "\n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Dataset Summary:\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Train Ratio: {TRAIN_RATIO}, Validation Ratio: {VAL_RATIO}, Test Ratio: {TEST_RATIO}\\n\")\n",
    "\n",
    "    # Print detailed split summary for all classes\n",
    "    print(\"Detailed Split Summary:\")\n",
    "    for class_name, counts in split_summary.items():\n",
    "        print(f\"{class_name} - Total: {counts['Total']}, Train: {counts['Train']}, Val: {counts['Validation']}, Test: {counts['Test']}\")\n",
    "\n",
    "    return split_summary\n",
    "\n",
    "# Run the split function and store the summary\n",
    "dataset_summary = analyze_and_split_dataset(original_dataset, split_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Augmentation Process ===\n",
      "\n",
      "Processing TRAIN split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 160 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 160/160 [00:00<00:00, 227.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 160/160 [00:32<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 68 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 68/68 [00:00<00:00, 177.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 68/68 [00:09<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 131 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 131/131 [00:00<00:00, 238.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 131/131 [00:13<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 77 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 77/77 [00:00<00:00, 243.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 77/77 [00:14<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 173 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 173/173 [00:00<00:00, 203.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 173/173 [01:06<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 124 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 124/124 [00:00<00:00, 207.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 124/124 [00:11<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 172 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 172/172 [00:00<00:00, 224.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 172/172 [00:26<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 149 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 149/149 [00:00<00:00, 191.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 149/149 [00:42<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VAL split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 20 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 20/20 [00:00<00:00, 213.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 20/20 [00:02<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 8 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 8/8 [00:00<00:00, 217.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 8/8 [00:00<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 16 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 16/16 [00:00<00:00, 249.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 16/16 [00:01<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 9 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 9/9 [00:00<00:00, 241.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 9/9 [00:03<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 21 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 21/21 [00:00<00:00, 233.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 21/21 [00:02<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 15 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 15/15 [00:00<00:00, 250.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 15/15 [00:01<00:00, 10.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 21 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 21/21 [00:00<00:00, 254.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 21/21 [00:02<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 18 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 18/18 [00:00<00:00, 222.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 18/18 [00:03<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TEST split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 20 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 20/20 [00:00<00:00, 221.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 20/20 [00:03<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 9 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 9/9 [00:00<00:00, 246.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 9/9 [00:00<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 17 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 17/17 [00:00<00:00, 217.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 17/17 [00:05<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 11 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 11/11 [00:00<00:00, 232.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 11/11 [00:01<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 224.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 0 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 16 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 16/16 [00:00<00:00, 215.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 16/16 [00:01<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 23 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 23/23 [00:00<00:00, 237.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 23/23 [00:02<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 20 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 20/20 [00:00<00:00, 234.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 20/20 [00:02<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Summary ===\n",
      "\n",
      "Per Split Statistics:\n",
      "\n",
      "TRAIN Split:\n",
      "Class 1:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 10:\n",
      "  Original: 160\n",
      "  Augmented: 1600\n",
      "  Total: 1760\n",
      "Class 100:\n",
      "  Original: 68\n",
      "  Augmented: 680\n",
      "  Total: 748\n",
      "Class 1000:\n",
      "  Original: 131\n",
      "  Augmented: 1310\n",
      "  Total: 1441\n",
      "Class 2:\n",
      "  Original: 77\n",
      "  Augmented: 770\n",
      "  Total: 847\n",
      "Class 20:\n",
      "  Original: 173\n",
      "  Augmented: 1730\n",
      "  Total: 1903\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 124\n",
      "  Augmented: 1240\n",
      "  Total: 1364\n",
      "Class 50:\n",
      "  Original: 172\n",
      "  Augmented: 1720\n",
      "  Total: 1892\n",
      "Class 500:\n",
      "  Original: 149\n",
      "  Augmented: 1490\n",
      "  Total: 1639\n",
      "\n",
      "TRAIN Split Totals:\n",
      "  Original Images: 1054\n",
      "  Augmented Images: 10540\n",
      "  Total Images: 11594\n",
      "--------------------------------------------------\n",
      "\n",
      "VAL Split:\n",
      "Class 1:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 10:\n",
      "  Original: 20\n",
      "  Augmented: 200\n",
      "  Total: 220\n",
      "Class 100:\n",
      "  Original: 8\n",
      "  Augmented: 80\n",
      "  Total: 88\n",
      "Class 1000:\n",
      "  Original: 16\n",
      "  Augmented: 160\n",
      "  Total: 176\n",
      "Class 2:\n",
      "  Original: 9\n",
      "  Augmented: 90\n",
      "  Total: 99\n",
      "Class 20:\n",
      "  Original: 21\n",
      "  Augmented: 210\n",
      "  Total: 231\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 15\n",
      "  Augmented: 150\n",
      "  Total: 165\n",
      "Class 50:\n",
      "  Original: 21\n",
      "  Augmented: 210\n",
      "  Total: 231\n",
      "Class 500:\n",
      "  Original: 18\n",
      "  Augmented: 180\n",
      "  Total: 198\n",
      "\n",
      "VAL Split Totals:\n",
      "  Original Images: 128\n",
      "  Augmented Images: 1280\n",
      "  Total Images: 1408\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST Split:\n",
      "Class 1:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 10:\n",
      "  Original: 20\n",
      "  Augmented: 200\n",
      "  Total: 220\n",
      "Class 100:\n",
      "  Original: 9\n",
      "  Augmented: 90\n",
      "  Total: 99\n",
      "Class 1000:\n",
      "  Original: 17\n",
      "  Augmented: 170\n",
      "  Total: 187\n",
      "Class 2:\n",
      "  Original: 11\n",
      "  Augmented: 110\n",
      "  Total: 121\n",
      "Class 20:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 200:\n",
      "  Original: 0\n",
      "  Augmented: 0\n",
      "  Total: 0\n",
      "Class 5:\n",
      "  Original: 16\n",
      "  Augmented: 160\n",
      "  Total: 176\n",
      "Class 50:\n",
      "  Original: 23\n",
      "  Augmented: 230\n",
      "  Total: 253\n",
      "Class 500:\n",
      "  Original: 20\n",
      "  Augmented: 200\n",
      "  Total: 220\n",
      "\n",
      "TEST Split Totals:\n",
      "  Original Images: 139\n",
      "  Augmented Images: 1390\n",
      "  Total Images: 1529\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Dataset Statistics:\n",
      "Total Original Images: 1321\n",
      "Total Augmented Images: 13210\n",
      "Total Images: 14531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-03\n",
    "# Code to apply augmentations to pre-split dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths - update these paths\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset3_split'  # Your already split dataset\n",
    "augmented_data_dir = '/scratch/movi/dm_project/data/split_80/dataset3_aug'  # Where to save augmented data\n",
    "\n",
    "# Number of augmentations per image\n",
    "NUM_AUGMENTATIONS = 10\n",
    "\n",
    "# Define augmentation transformations\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "def apply_augmentations():\n",
    "    \"\"\"Apply augmentations to each split of the pre-split dataset.\"\"\"\n",
    "    print(\"\\n=== Starting Augmentation Process ===\")\n",
    "    \n",
    "    # Create destination directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(augmented_data_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for class_name in os.listdir(os.path.join(split_base_dir, split)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "    # Stats dictionary\n",
    "    stats = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "    # Process each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split:\")\n",
    "        split_source = os.path.join(split_base_dir, split)\n",
    "        split_dest = os.path.join(augmented_data_dir, split)\n",
    "        \n",
    "        # Process each class\n",
    "        for class_name in sorted(os.listdir(split_source)):\n",
    "            class_source = os.path.join(split_source, class_name)\n",
    "            class_dest = os.path.join(split_dest, class_name)\n",
    "            \n",
    "            if os.path.isdir(class_source):\n",
    "                # Get list of original images\n",
    "                original_files = [f for f in os.listdir(class_source) \n",
    "                                if os.path.isfile(os.path.join(class_source, f))]\n",
    "                \n",
    "                print(f\"\\nProcessing class {class_name}:\")\n",
    "                print(f\"Found {len(original_files)} original images\")\n",
    "                \n",
    "                # First copy original files\n",
    "                for file in tqdm(original_files, desc=\"Copying originals\"):\n",
    "                    shutil.copy2(os.path.join(class_source, file),\n",
    "                               os.path.join(class_dest, file))\n",
    "                \n",
    "                # Then create augmented versions\n",
    "                print(\"Generating augmented images...\")\n",
    "                for file in tqdm(original_files, desc=\"Generating augmentations\"):\n",
    "                    img_path = os.path.join(class_source, file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            # Convert to RGB if needed\n",
    "                            if img.mode != 'RGB':\n",
    "                                img = img.convert('RGB')\n",
    "                            \n",
    "                            img_tensor = transforms.ToTensor()(img)\n",
    "                            \n",
    "                            # Generate augmentations\n",
    "                            for i in range(NUM_AUGMENTATIONS):\n",
    "                                try:\n",
    "                                    augmented_tensor = augmentation_transforms(img_tensor)\n",
    "                                    augmented_img = transforms.ToPILImage()(augmented_tensor)\n",
    "                                    \n",
    "                                    # Save augmented image\n",
    "                                    base_name = os.path.splitext(file)[0]\n",
    "                                    aug_name = f\"{base_name}_aug_{i+1}.jpg\"\n",
    "                                    augmented_img.save(os.path.join(class_dest, aug_name))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error generating augmentation {i+1} for {file}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file}: {str(e)}\")\n",
    "                \n",
    "                # Update stats\n",
    "                total_augmented = len(original_files) * NUM_AUGMENTATIONS\n",
    "                stats[split][class_name] = {\n",
    "                    'original': len(original_files),\n",
    "                    'augmented': total_augmented,\n",
    "                    'total': len(original_files) + total_augmented\n",
    "                }\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n=== Augmentation Summary ===\")\n",
    "    print(\"\\nPer Split Statistics:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        split_total_orig = 0\n",
    "        split_total_aug = 0\n",
    "        \n",
    "        for class_name, counts in sorted(stats[split].items()):\n",
    "            print(f\"Class {class_name}:\")\n",
    "            print(f\"  Original: {counts['original']}\")\n",
    "            print(f\"  Augmented: {counts['augmented']}\")\n",
    "            print(f\"  Total: {counts['total']}\")\n",
    "            split_total_orig += counts['original']\n",
    "            split_total_aug += counts['augmented']\n",
    "        \n",
    "        print(f\"\\n{split.upper()} Split Totals:\")\n",
    "        print(f\"  Original Images: {split_total_orig}\")\n",
    "        print(f\"  Augmented Images: {split_total_aug}\")\n",
    "        print(f\"  Total Images: {split_total_orig + split_total_aug}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Overall totals\n",
    "    total_orig = sum(sum(c['original'] for c in s.values()) for s in stats.values())\n",
    "    total_aug = sum(sum(c['augmented'] for c in s.values()) for s in stats.values())\n",
    "    print(\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"Total Original Images: {total_orig}\")\n",
    "    print(f\"Total Augmented Images: {total_aug}\")\n",
    "    print(f\"Total Images: {total_orig + total_aug}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_augmentations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 28 train, 3 val, 5 test (Total: 36)\n",
      "10: 284 train, 35 val, 36 test (Total: 355)\n",
      "100: 196 train, 24 val, 26 test (Total: 246)\n",
      "1000: 200 train, 25 val, 25 test (Total: 250)\n",
      "2: 211 train, 26 val, 27 test (Total: 264)\n",
      "20: 289 train, 36 val, 37 test (Total: 362)\n",
      "200: 15 train, 1 val, 3 test (Total: 19)\n",
      "5: 237 train, 29 val, 31 test (Total: 297)\n",
      "50: 284 train, 35 val, 36 test (Total: 355)\n",
      "500: 238 train, 29 val, 31 test (Total: 298)\n",
      "\n",
      "Overall Dataset Summary:\n",
      "Total Images: 2482\n",
      "Train Ratio: 0.8, Validation Ratio: 0.1, Test Ratio: 0.1\n",
      "\n",
      "Detailed Split Summary:\n",
      "1 - Total: 36, Train: 28, Val: 3, Test: 5\n",
      "10 - Total: 355, Train: 284, Val: 35, Test: 36\n",
      "100 - Total: 246, Train: 196, Val: 24, Test: 26\n",
      "1000 - Total: 250, Train: 200, Val: 25, Test: 25\n",
      "2 - Total: 264, Train: 211, Val: 26, Test: 27\n",
      "20 - Total: 362, Train: 289, Val: 36, Test: 37\n",
      "200 - Total: 19, Train: 15, Val: 1, Test: 3\n",
      "5 - Total: 297, Train: 237, Val: 29, Test: 31\n",
      "50 - Total: 355, Train: 284, Val: 35, Test: 36\n",
      "500 - Total: 298, Train: 238, Val: 29, Test: 31\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi\n",
    "# Code to split dataset into train, validation, and test sets, with detailed analysis and logging\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "original_dataset = '/scratch/movi/dm_project/data/dataset_combined_unique'  # Replace with the path to your original dataset\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset_combined_split'        # Base directory to store train/val/test splits\n",
    "\n",
    "# Split ratios\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "def create_dir_structure(base_dir, class_names):\n",
    "    \"\"\"Create train, val, and test directories for each class.\"\"\"\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for class_name in class_names:\n",
    "            os.makedirs(os.path.join(base_dir, split, class_name), exist_ok=True)\n",
    "\n",
    "def analyze_and_split_dataset(original_dataset, split_base_dir):\n",
    "    \"\"\"Analyze dataset and split into train, val, and test sets.\"\"\"\n",
    "    class_names = sorted(os.listdir(original_dataset))  # Get class names in alphabetical order\n",
    "    create_dir_structure(split_base_dir, class_names)   # Create the necessary directory structure\n",
    "\n",
    "    total_images = 0  # Track the total number of images across all classes\n",
    "    split_summary = {}  # Dictionary to store per-class split details\n",
    "\n",
    "    # Loop through each class folder\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(original_dataset, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):  # Ensure it's a folder\n",
    "            # List all images in the class folder\n",
    "            image_files = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))]\n",
    "            random.shuffle(image_files)  # Shuffle images to ensure randomness\n",
    "\n",
    "            # Calculate split indices\n",
    "            total_images_in_class = len(image_files)\n",
    "            train_end = int(total_images_in_class * TRAIN_RATIO)\n",
    "            val_end = train_end + int(total_images_in_class * VAL_RATIO)\n",
    "\n",
    "            # Split the image files into train, val, and test sets\n",
    "            train_files = image_files[:train_end]\n",
    "            val_files = image_files[train_end:val_end]\n",
    "            test_files = image_files[val_end:]\n",
    "\n",
    "            # Copy files to the respective split directories\n",
    "            for file in train_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'train', class_name, file))\n",
    "            for file in val_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'val', class_name, file))\n",
    "            for file in test_files:\n",
    "                shutil.copy(os.path.join(class_path, file), os.path.join(split_base_dir, 'test', class_name, file))\n",
    "\n",
    "            # Store the split summary for this class\n",
    "            split_summary[class_name] = {\n",
    "                'Total': total_images_in_class,\n",
    "                'Train': len(train_files),\n",
    "                'Validation': len(val_files),\n",
    "                'Test': len(test_files)\n",
    "            }\n",
    "\n",
    "            # Update total image count\n",
    "            total_images += total_images_in_class\n",
    "\n",
    "            # Print per-class summary\n",
    "            print(f\"{class_name}: {len(train_files)} train, {len(val_files)} val, {len(test_files)} test (Total: {total_images_in_class})\")\n",
    "\n",
    "    # Print overall summary\n",
    "    print(\"\\nOverall Dataset Summary:\")\n",
    "    print(f\"Total Images: {total_images}\")\n",
    "    print(f\"Train Ratio: {TRAIN_RATIO}, Validation Ratio: {VAL_RATIO}, Test Ratio: {TEST_RATIO}\\n\")\n",
    "\n",
    "    # Print detailed split summary for all classes\n",
    "    print(\"Detailed Split Summary:\")\n",
    "    for class_name, counts in split_summary.items():\n",
    "        print(f\"{class_name} - Total: {counts['Total']}, Train: {counts['Train']}, Val: {counts['Validation']}, Test: {counts['Test']}\")\n",
    "\n",
    "    return split_summary\n",
    "\n",
    "# Run the split function and store the summary\n",
    "dataset_summary = analyze_and_split_dataset(original_dataset, split_base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Augmentation Process ===\n",
      "\n",
      "Processing TRAIN split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 28 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 28/28 [00:00<00:00, 215.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 28/28 [00:02<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 284 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 284/284 [00:01<00:00, 212.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 284/284 [01:03<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 196 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 196/196 [00:00<00:00, 210.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 196/196 [01:16<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 200 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 200/200 [00:01<00:00, 181.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 200/200 [01:30<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 211 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 211/211 [00:00<00:00, 245.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 211/211 [00:59<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 289 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 289/289 [00:01<00:00, 213.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 289/289 [00:44<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 15 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 15/15 [00:00<00:00, 239.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 15/15 [00:01<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 237 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 237/237 [00:01<00:00, 211.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 237/237 [01:09<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 284 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 284/284 [00:02<00:00, 110.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 284/284 [01:50<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 238 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 238/238 [00:01<00:00, 205.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 238/238 [00:30<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing VAL split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 3 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 3/3 [00:00<00:00, 129.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 3/3 [00:00<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 35 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 35/35 [00:00<00:00, 145.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 35/35 [00:05<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 24 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 24/24 [00:00<00:00, 233.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 24/24 [00:02<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 203.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:02<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 26 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 26/26 [00:00<00:00, 255.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 26/26 [00:02<00:00,  9.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 36 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 36/36 [00:00<00:00, 229.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 36/36 [00:20<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 1 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 1/1 [00:00<00:00, 211.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 1/1 [00:00<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 29 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 29/29 [00:00<00:00, 161.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 29/29 [00:05<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 35 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 35/35 [00:00<00:00, 232.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 35/35 [00:03<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 29 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 29/29 [00:00<00:00, 146.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 29/29 [00:02<00:00, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing TEST split:\n",
      "\n",
      "Processing class 1:\n",
      "Found 5 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 5/5 [00:00<00:00, 236.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 5/5 [00:00<00:00, 11.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 10:\n",
      "Found 36 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 36/36 [00:00<00:00, 184.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 36/36 [00:03<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 100:\n",
      "Found 26 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 26/26 [00:00<00:00, 219.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 26/26 [00:37<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 1000:\n",
      "Found 25 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 25/25 [00:00<00:00, 227.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 25/25 [00:02<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 2:\n",
      "Found 27 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 27/27 [00:00<00:00, 215.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 27/27 [00:02<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 20:\n",
      "Found 37 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 37/37 [00:00<00:00, 182.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 37/37 [00:07<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 200:\n",
      "Found 3 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 3/3 [00:00<00:00, 213.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 3/3 [00:00<00:00,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 5:\n",
      "Found 31 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 31/31 [00:00<00:00, 166.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 31/31 [00:03<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 50:\n",
      "Found 36 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 36/36 [00:00<00:00, 233.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 36/36 [00:03<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing class 500:\n",
      "Found 31 original images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying originals: 100%|██████████| 31/31 [00:00<00:00, 187.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating augmented images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmentations: 100%|██████████| 31/31 [00:04<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Augmentation Summary ===\n",
      "\n",
      "Per Split Statistics:\n",
      "\n",
      "TRAIN Split:\n",
      "Class 1:\n",
      "  Original: 28\n",
      "  Augmented: 280\n",
      "  Total: 308\n",
      "Class 10:\n",
      "  Original: 284\n",
      "  Augmented: 2840\n",
      "  Total: 3124\n",
      "Class 100:\n",
      "  Original: 196\n",
      "  Augmented: 1960\n",
      "  Total: 2156\n",
      "Class 1000:\n",
      "  Original: 200\n",
      "  Augmented: 2000\n",
      "  Total: 2200\n",
      "Class 2:\n",
      "  Original: 211\n",
      "  Augmented: 2110\n",
      "  Total: 2321\n",
      "Class 20:\n",
      "  Original: 289\n",
      "  Augmented: 2890\n",
      "  Total: 3179\n",
      "Class 200:\n",
      "  Original: 15\n",
      "  Augmented: 150\n",
      "  Total: 165\n",
      "Class 5:\n",
      "  Original: 237\n",
      "  Augmented: 2370\n",
      "  Total: 2607\n",
      "Class 50:\n",
      "  Original: 284\n",
      "  Augmented: 2840\n",
      "  Total: 3124\n",
      "Class 500:\n",
      "  Original: 238\n",
      "  Augmented: 2380\n",
      "  Total: 2618\n",
      "\n",
      "TRAIN Split Totals:\n",
      "  Original Images: 1982\n",
      "  Augmented Images: 19820\n",
      "  Total Images: 21802\n",
      "--------------------------------------------------\n",
      "\n",
      "VAL Split:\n",
      "Class 1:\n",
      "  Original: 3\n",
      "  Augmented: 30\n",
      "  Total: 33\n",
      "Class 10:\n",
      "  Original: 35\n",
      "  Augmented: 350\n",
      "  Total: 385\n",
      "Class 100:\n",
      "  Original: 24\n",
      "  Augmented: 240\n",
      "  Total: 264\n",
      "Class 1000:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 2:\n",
      "  Original: 26\n",
      "  Augmented: 260\n",
      "  Total: 286\n",
      "Class 20:\n",
      "  Original: 36\n",
      "  Augmented: 360\n",
      "  Total: 396\n",
      "Class 200:\n",
      "  Original: 1\n",
      "  Augmented: 10\n",
      "  Total: 11\n",
      "Class 5:\n",
      "  Original: 29\n",
      "  Augmented: 290\n",
      "  Total: 319\n",
      "Class 50:\n",
      "  Original: 35\n",
      "  Augmented: 350\n",
      "  Total: 385\n",
      "Class 500:\n",
      "  Original: 29\n",
      "  Augmented: 290\n",
      "  Total: 319\n",
      "\n",
      "VAL Split Totals:\n",
      "  Original Images: 243\n",
      "  Augmented Images: 2430\n",
      "  Total Images: 2673\n",
      "--------------------------------------------------\n",
      "\n",
      "TEST Split:\n",
      "Class 1:\n",
      "  Original: 5\n",
      "  Augmented: 50\n",
      "  Total: 55\n",
      "Class 10:\n",
      "  Original: 36\n",
      "  Augmented: 360\n",
      "  Total: 396\n",
      "Class 100:\n",
      "  Original: 26\n",
      "  Augmented: 260\n",
      "  Total: 286\n",
      "Class 1000:\n",
      "  Original: 25\n",
      "  Augmented: 250\n",
      "  Total: 275\n",
      "Class 2:\n",
      "  Original: 27\n",
      "  Augmented: 270\n",
      "  Total: 297\n",
      "Class 20:\n",
      "  Original: 37\n",
      "  Augmented: 370\n",
      "  Total: 407\n",
      "Class 200:\n",
      "  Original: 3\n",
      "  Augmented: 30\n",
      "  Total: 33\n",
      "Class 5:\n",
      "  Original: 31\n",
      "  Augmented: 310\n",
      "  Total: 341\n",
      "Class 50:\n",
      "  Original: 36\n",
      "  Augmented: 360\n",
      "  Total: 396\n",
      "Class 500:\n",
      "  Original: 31\n",
      "  Augmented: 310\n",
      "  Total: 341\n",
      "\n",
      "TEST Split Totals:\n",
      "  Original Images: 257\n",
      "  Augmented Images: 2570\n",
      "  Total Images: 2827\n",
      "--------------------------------------------------\n",
      "\n",
      "Overall Dataset Statistics:\n",
      "Total Original Images: 2482\n",
      "Total Augmented Images: 24820\n",
      "Total Images: 27302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-03\n",
    "# Code to apply augmentations to pre-split dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths - update these paths\n",
    "split_base_dir = '/scratch/movi/dm_project/data/split_80/dataset_combined_split'  # Your already split dataset\n",
    "augmented_data_dir = '/scratch/movi/dm_project/data/custom/dataset_combined_aug'  # Where to save augmented data\n",
    "\n",
    "# Number of augmentations per image\n",
    "NUM_AUGMENTATIONS = 10\n",
    "\n",
    "# Define augmentation transformations\n",
    "augmentation_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
    "])\n",
    "\n",
    "def apply_augmentations():\n",
    "    \"\"\"Apply augmentations to each split of the pre-split dataset.\"\"\"\n",
    "    print(\"\\n=== Starting Augmentation Process ===\")\n",
    "    \n",
    "    # Create destination directory structure\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = os.path.join(augmented_data_dir, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for class_name in os.listdir(os.path.join(split_base_dir, split)):\n",
    "            class_path = os.path.join(split_path, class_name)\n",
    "            os.makedirs(class_path, exist_ok=True)\n",
    "\n",
    "    # Stats dictionary\n",
    "    stats = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "    # Process each split\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\nProcessing {split.upper()} split:\")\n",
    "        split_source = os.path.join(split_base_dir, split)\n",
    "        split_dest = os.path.join(augmented_data_dir, split)\n",
    "        \n",
    "        # Process each class\n",
    "        for class_name in sorted(os.listdir(split_source)):\n",
    "            class_source = os.path.join(split_source, class_name)\n",
    "            class_dest = os.path.join(split_dest, class_name)\n",
    "            \n",
    "            if os.path.isdir(class_source):\n",
    "                # Get list of original images\n",
    "                original_files = [f for f in os.listdir(class_source) \n",
    "                                if os.path.isfile(os.path.join(class_source, f))]\n",
    "                \n",
    "                print(f\"\\nProcessing class {class_name}:\")\n",
    "                print(f\"Found {len(original_files)} original images\")\n",
    "                \n",
    "                # First copy original files\n",
    "                for file in tqdm(original_files, desc=\"Copying originals\"):\n",
    "                    shutil.copy2(os.path.join(class_source, file),\n",
    "                               os.path.join(class_dest, file))\n",
    "                \n",
    "                # Then create augmented versions\n",
    "                print(\"Generating augmented images...\")\n",
    "                for file in tqdm(original_files, desc=\"Generating augmentations\"):\n",
    "                    img_path = os.path.join(class_source, file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            # Convert to RGB if needed\n",
    "                            if img.mode != 'RGB':\n",
    "                                img = img.convert('RGB')\n",
    "                            \n",
    "                            img_tensor = transforms.ToTensor()(img)\n",
    "                            \n",
    "                            # Generate augmentations\n",
    "                            for i in range(NUM_AUGMENTATIONS):\n",
    "                                try:\n",
    "                                    augmented_tensor = augmentation_transforms(img_tensor)\n",
    "                                    augmented_img = transforms.ToPILImage()(augmented_tensor)\n",
    "                                    \n",
    "                                    # Save augmented image\n",
    "                                    base_name = os.path.splitext(file)[0]\n",
    "                                    aug_name = f\"{base_name}_aug_{i+1}.jpg\"\n",
    "                                    augmented_img.save(os.path.join(class_dest, aug_name))\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error generating augmentation {i+1} for {file}: {str(e)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing file {file}: {str(e)}\")\n",
    "                \n",
    "                # Update stats\n",
    "                total_augmented = len(original_files) * NUM_AUGMENTATIONS\n",
    "                stats[split][class_name] = {\n",
    "                    'original': len(original_files),\n",
    "                    'augmented': total_augmented,\n",
    "                    'total': len(original_files) + total_augmented\n",
    "                }\n",
    "\n",
    "    # Print comprehensive summary\n",
    "    print(\"\\n=== Augmentation Summary ===\")\n",
    "    print(\"\\nPer Split Statistics:\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{split.upper()} Split:\")\n",
    "        split_total_orig = 0\n",
    "        split_total_aug = 0\n",
    "        \n",
    "        for class_name, counts in sorted(stats[split].items()):\n",
    "            print(f\"Class {class_name}:\")\n",
    "            print(f\"  Original: {counts['original']}\")\n",
    "            print(f\"  Augmented: {counts['augmented']}\")\n",
    "            print(f\"  Total: {counts['total']}\")\n",
    "            split_total_orig += counts['original']\n",
    "            split_total_aug += counts['augmented']\n",
    "        \n",
    "        print(f\"\\n{split.upper()} Split Totals:\")\n",
    "        print(f\"  Original Images: {split_total_orig}\")\n",
    "        print(f\"  Augmented Images: {split_total_aug}\")\n",
    "        print(f\"  Total Images: {split_total_orig + split_total_aug}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Overall totals\n",
    "    total_orig = sum(sum(c['original'] for c in s.values()) for s in stats.values())\n",
    "    total_aug = sum(sum(c['augmented'] for c in s.values()) for s in stats.values())\n",
    "    print(\"\\nOverall Dataset Statistics:\")\n",
    "    print(f\"Total Original Images: {total_orig}\")\n",
    "    print(f\"Total Augmented Images: {total_aug}\")\n",
    "    print(f\"Total Images: {total_orig + total_aug}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    apply_augmentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END of Data Split and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
