{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Pipeline Summary\n",
    "\n",
    "## 1. Datasets and Models Used\n",
    "   - **Datasets**:\n",
    "     - 4 datasets in total: three individual datasets and one combined dataset.\n",
    "   - **Model**:\n",
    "     - Custom lightweight CNN model with:\n",
    "       - **Convolutional Layers**: Three layers with Batch Normalization, ReLU activation, and MaxPooling for spatial feature extraction.\n",
    "       - **Fully Connected Layers**: Two layers with Batch Normalization and ReLU in the hidden layer.\n",
    "       - **Output Layer**: Final fully connected layer for classification into 9 classes.\n",
    "\n",
    "## 2. Experiment Setup\n",
    "   - **Hyperparameter Ranges**:\n",
    "     - **Batch Size**: [16, 32, 64, 128].\n",
    "     - **Learning Rate**: [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005].\n",
    "     - **Epochs**: [15, 25, 35, 50].\n",
    "   - Selected optimal hyperparameters for each model based on validation performance.\n",
    "\n",
    "## 3. Training and Validation Process\n",
    "   - **Training Loop**:\n",
    "     - Optimized models over multiple epochs for various hyperparameter combinations.\n",
    "     - Logged training and validation accuracy and loss per epoch.\n",
    "     - Tracked total training time for each model.\n",
    "   - **Validation**:\n",
    "     - Monitored performance on validation data to track generalization and prevent overfitting.\n",
    "\n",
    "## 4. Preprocessing Steps\n",
    "   - **Image Enhancements**:\n",
    "     - Applied Median Blur, Basic Sharpening, and Contrast Stretching.\n",
    "     - Used CLAHE (Contrast Limited Adaptive Histogram Equalization) for enhanced contrast.\n",
    "   - **Transformations**:\n",
    "     - Resized images to 64x64.\n",
    "     - Applied Random Horizontal Flip for data augmentation.\n",
    "     - Normalized pixel values to mean `[0.485, 0.456, 0.406]` and standard deviation `[0.229, 0.224, 0.225]`.\n",
    "\n",
    "## 5. Evaluation Metrics and Visualizations\n",
    "   - **Test Set Evaluation**:\n",
    "     - Assessed using Macro-averaged F1 score, precision, recall, and per-class F1 scores.\n",
    "   - **Visualizations**:\n",
    "     - Confusion Matrix for class-wise prediction analysis.\n",
    "     - Classification Report Heatmap with precision, recall, and F1 scores.\n",
    "     - ROC Curves for multi-class AUC (Area Under the Curve) evaluation.\n",
    "\n",
    "## 6. Key Outputs for Each Dataset-Model Combination\n",
    "   - **Training and Validation Curves**:\n",
    "     - Generated and saved plots for training and validation accuracy and loss.\n",
    "   - **Model State Saving**:\n",
    "     - Saved trained model states for potential future use.\n",
    "   - **Detailed Metrics Visualizations**:\n",
    "     - Produced classification reports, confusion matrices, and ROC curves for comprehensive performance analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Virtual Environment</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /scratch/movi/dmp/lib/python3.9/site-packages (6.29.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (1.8.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (8.18.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing-extensions in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (8.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.6)\n",
      "Requirement already satisfied: zipp>=3.20 in /scratch/movi/dmp/lib/python3.9/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel) (3.20.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /scratch/movi/dmp/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /scratch/movi/dmp/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/movi/dmp/lib/python3.9/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /scratch/movi/dmp/lib/python3.9/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec dmp in /home/movi/.local/share/jupyter/kernels/dmp\n"
     ]
    }
   ],
   "source": [
    "# # Create the virtual environment named 'dmp'\n",
    "!python3 -m venv /scratch/movi/dmp\n",
    "# Install ipykernel inside the 'dmp' environment\n",
    "!/scratch/movi/dmp/bin/pip install ipykernel\n",
    "# Add 'dmp' as a kernel for Jupyter Notebook\n",
    "!/scratch/movi/dmp/bin/python -m ipykernel install --user --name=dmp --display-name \"Python (dmp)\"\n",
    "# # Upgrade pip in the 'dmp' environment\n",
    "# !/scratch/movi/dmp/bin/python3 -m pip install --upgrade pip\n",
    "# # Install necessary packages (NumPy, PyTorch, etc.) inside 'dmp'\n",
    "# !/scratch/movi/dmp/bin/pip install numpy torch torchvision torchaudio pandas matplotlib scikit-learn\n",
    "# !pip install numpy==1.21.4 scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.9.9 (main, Mar 25 2022, 16:08:31) \n",
      "[GCC 10.3.0]\n",
      "NumPy Version: 1.21.4\n",
      "PyTorch Version: 1.12.1+cu113\n",
      "CUDA is available. PyTorch is using GPU.\n",
      "\n",
      "Number of GPUs available: 1\n",
      "\n",
      "GPU 0: NVIDIA A100-SXM4-80GB MIG 3g.40gb\n",
      "  Total Memory: 39.25 GB\n",
      "  Memory Allocated: 0.00 GB\n",
      "  Memory Reserved (Cached): 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# Prints the installed versions of Python, NumPy, and PyTorch libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Function to check GPU availability and display memory statistics using PyTorch's CUDA interface\n",
    "def check_gpu_status():\n",
    "    # Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA is available. PyTorch is using GPU.\\n\")\n",
    "        # Get the number of available GPUs\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Number of GPUs available: {num_gpus}\")\n",
    "        # Loop through each GPU and display its details\n",
    "        for gpu_id in range(num_gpus):\n",
    "            gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "            gpu_memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024 ** 3)  # In GB\n",
    "            gpu_memory_cached = torch.cuda.memory_reserved(gpu_id) / (1024 ** 3)      # In GB\n",
    "            gpu_memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / (1024 ** 3)  # In GB\n",
    "            print(f\"\\nGPU {gpu_id}: {gpu_name}\")\n",
    "            print(f\"  Total Memory: {gpu_memory_total:.2f} GB\")\n",
    "            print(f\"  Memory Allocated: {gpu_memory_allocated:.2f} GB\")\n",
    "            print(f\"  Memory Reserved (Cached): {gpu_memory_cached:.2f} GB\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. PyTorch is using the CPU.\")\n",
    "\n",
    "# Run the GPU status check\n",
    "check_gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 20:15:35,416 - Training dataset size: 10021\n",
      "2024-11-21 20:15:35,416 - Validation dataset size: 1199\n",
      "2024-11-21 20:15:35,417 - Test dataset size: 1342\n",
      "2024-11-21 20:15:35,419 - Training with Batch Size: 128, Learning Rate: 0.0001, Epochs: 25\n",
      "2024-11-21 20:16:24,344 - Epoch 1/25 - Training: Loss = 1.7224, Accuracy = 0.4370\n",
      "2024-11-21 20:16:30,288 - Epoch 1/25 - Validation: Loss = 1.5037, Accuracy = 0.5338\n",
      "2024-11-21 20:16:30,288 - Time for epoch 1: 54.86s\n",
      "2024-11-21 20:17:16,663 - Epoch 2/25 - Training: Loss = 1.3171, Accuracy = 0.6639\n",
      "2024-11-21 20:17:22,222 - Epoch 2/25 - Validation: Loss = 1.2348, Accuracy = 0.7014\n",
      "2024-11-21 20:17:22,223 - Time for epoch 2: 51.93s\n",
      "2024-11-21 20:18:06,576 - Epoch 3/25 - Training: Loss = 1.0821, Accuracy = 0.7674\n",
      "2024-11-21 20:18:12,684 - Epoch 3/25 - Validation: Loss = 1.0553, Accuracy = 0.7623\n",
      "2024-11-21 20:18:12,684 - Time for epoch 3: 50.46s\n",
      "2024-11-21 20:18:56,624 - Epoch 4/25 - Training: Loss = 0.9072, Accuracy = 0.8248\n",
      "2024-11-21 20:19:01,565 - Epoch 4/25 - Validation: Loss = 0.9175, Accuracy = 0.8215\n",
      "2024-11-21 20:19:01,566 - Time for epoch 4: 48.88s\n",
      "2024-11-21 20:19:49,238 - Epoch 5/25 - Training: Loss = 0.7695, Accuracy = 0.8648\n",
      "2024-11-21 20:19:54,659 - Epoch 5/25 - Validation: Loss = 0.8065, Accuracy = 0.8382\n",
      "2024-11-21 20:19:54,660 - Time for epoch 5: 53.09s\n",
      "2024-11-21 20:20:40,408 - Epoch 6/25 - Training: Loss = 0.6590, Accuracy = 0.8921\n",
      "2024-11-21 20:20:45,503 - Epoch 6/25 - Validation: Loss = 0.7016, Accuracy = 0.8699\n",
      "2024-11-21 20:20:45,504 - Time for epoch 6: 50.84s\n",
      "2024-11-21 20:21:31,893 - Epoch 7/25 - Training: Loss = 0.5659, Accuracy = 0.9120\n",
      "2024-11-21 20:21:36,739 - Epoch 7/25 - Validation: Loss = 0.6338, Accuracy = 0.8724\n",
      "2024-11-21 20:21:36,739 - Time for epoch 7: 51.23s\n",
      "2024-11-21 20:22:20,494 - Epoch 8/25 - Training: Loss = 0.4898, Accuracy = 0.9329\n",
      "2024-11-21 20:22:25,549 - Epoch 8/25 - Validation: Loss = 0.5558, Accuracy = 0.8849\n",
      "2024-11-21 20:22:25,550 - Time for epoch 8: 48.81s\n",
      "2024-11-21 20:23:09,204 - Epoch 9/25 - Training: Loss = 0.4242, Accuracy = 0.9477\n",
      "2024-11-21 20:23:14,967 - Epoch 9/25 - Validation: Loss = 0.5290, Accuracy = 0.8916\n",
      "2024-11-21 20:23:14,967 - Time for epoch 9: 49.42s\n",
      "2024-11-21 20:23:58,811 - Epoch 10/25 - Training: Loss = 0.3685, Accuracy = 0.9559\n",
      "2024-11-21 20:24:03,948 - Epoch 10/25 - Validation: Loss = 0.4777, Accuracy = 0.9041\n",
      "2024-11-21 20:24:03,949 - Time for epoch 10: 48.98s\n",
      "2024-11-21 20:24:46,881 - Epoch 11/25 - Training: Loss = 0.3193, Accuracy = 0.9658\n",
      "2024-11-21 20:24:51,866 - Epoch 11/25 - Validation: Loss = 0.4469, Accuracy = 0.9124\n",
      "2024-11-21 20:24:51,868 - Time for epoch 11: 47.92s\n",
      "2024-11-21 20:25:35,700 - Epoch 12/25 - Training: Loss = 0.2795, Accuracy = 0.9749\n",
      "2024-11-21 20:25:41,323 - Epoch 12/25 - Validation: Loss = 0.4072, Accuracy = 0.9158\n",
      "2024-11-21 20:25:41,323 - Time for epoch 12: 49.45s\n",
      "2024-11-21 20:26:27,213 - Epoch 13/25 - Training: Loss = 0.2430, Accuracy = 0.9796\n",
      "2024-11-21 20:26:32,526 - Epoch 13/25 - Validation: Loss = 0.3778, Accuracy = 0.9258\n",
      "2024-11-21 20:26:32,527 - Time for epoch 13: 51.20s\n",
      "2024-11-21 20:27:16,768 - Epoch 14/25 - Training: Loss = 0.2136, Accuracy = 0.9850\n",
      "2024-11-21 20:27:23,058 - Epoch 14/25 - Validation: Loss = 0.3594, Accuracy = 0.9191\n",
      "2024-11-21 20:27:23,059 - Time for epoch 14: 50.53s\n",
      "2024-11-21 20:28:06,579 - Epoch 15/25 - Training: Loss = 0.1881, Accuracy = 0.9883\n",
      "2024-11-21 20:28:11,767 - Epoch 15/25 - Validation: Loss = 0.3531, Accuracy = 0.9216\n",
      "2024-11-21 20:28:11,767 - Time for epoch 15: 48.71s\n",
      "2024-11-21 20:28:56,325 - Epoch 16/25 - Training: Loss = 0.1668, Accuracy = 0.9914\n",
      "2024-11-21 20:29:01,245 - Epoch 16/25 - Validation: Loss = 0.3342, Accuracy = 0.9191\n",
      "2024-11-21 20:29:01,246 - Time for epoch 16: 49.48s\n",
      "2024-11-21 20:29:46,035 - Epoch 17/25 - Training: Loss = 0.1462, Accuracy = 0.9928\n",
      "2024-11-21 20:29:50,913 - Epoch 17/25 - Validation: Loss = 0.3073, Accuracy = 0.9308\n",
      "2024-11-21 20:29:50,914 - Time for epoch 17: 49.67s\n",
      "2024-11-21 20:30:33,991 - Epoch 18/25 - Training: Loss = 0.1293, Accuracy = 0.9960\n",
      "2024-11-21 20:30:39,437 - Epoch 18/25 - Validation: Loss = 0.2935, Accuracy = 0.9366\n",
      "2024-11-21 20:30:39,438 - Time for epoch 18: 48.52s\n",
      "2024-11-21 20:31:24,591 - Epoch 19/25 - Training: Loss = 0.1162, Accuracy = 0.9964\n",
      "2024-11-21 20:31:29,523 - Epoch 19/25 - Validation: Loss = 0.2835, Accuracy = 0.9341\n",
      "2024-11-21 20:31:29,525 - Time for epoch 19: 50.09s\n",
      "2024-11-21 20:32:16,184 - Epoch 20/25 - Training: Loss = 0.1014, Accuracy = 0.9980\n",
      "2024-11-21 20:32:22,006 - Epoch 20/25 - Validation: Loss = 0.2743, Accuracy = 0.9366\n",
      "2024-11-21 20:32:22,007 - Time for epoch 20: 52.48s\n",
      "2024-11-21 20:33:06,534 - Epoch 21/25 - Training: Loss = 0.0934, Accuracy = 0.9977\n",
      "2024-11-21 20:33:11,392 - Epoch 21/25 - Validation: Loss = 0.2724, Accuracy = 0.9349\n",
      "2024-11-21 20:33:11,393 - Time for epoch 21: 49.38s\n",
      "2024-11-21 20:33:55,334 - Epoch 22/25 - Training: Loss = 0.0837, Accuracy = 0.9988\n",
      "2024-11-21 20:34:00,230 - Epoch 22/25 - Validation: Loss = 0.2568, Accuracy = 0.9374\n",
      "2024-11-21 20:34:00,231 - Time for epoch 22: 48.84s\n",
      "2024-11-21 20:34:42,673 - Epoch 23/25 - Training: Loss = 0.0747, Accuracy = 0.9988\n",
      "2024-11-21 20:34:49,014 - Epoch 23/25 - Validation: Loss = 0.2394, Accuracy = 0.9441\n",
      "2024-11-21 20:34:49,015 - Time for epoch 23: 48.78s\n",
      "2024-11-21 20:35:31,948 - Epoch 24/25 - Training: Loss = 0.0675, Accuracy = 0.9993\n",
      "2024-11-21 20:35:36,998 - Epoch 24/25 - Validation: Loss = 0.2356, Accuracy = 0.9399\n",
      "2024-11-21 20:35:36,999 - Time for epoch 24: 47.98s\n",
      "2024-11-21 20:36:20,770 - Epoch 25/25 - Training: Loss = 0.0613, Accuracy = 0.9993\n",
      "2024-11-21 20:36:25,665 - Epoch 25/25 - Validation: Loss = 0.2354, Accuracy = 0.9408\n",
      "2024-11-21 20:36:25,666 - Time for epoch 25: 48.67s\n",
      "2024-11-21 20:36:25,666 - Total Training Time: 1250.20s\n",
      "2024-11-21 20:36:31,979 - Macro-Averaged F1 Score: 0.9238\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-21\n",
    "# Image classification pipeline using a custom simple CNN model and enhanced preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "train_dir = '/scratch/movi/dm_project/data/split_80/dataset1_aug/train'\n",
    "val_dir = '/scratch/movi/dm_project/data/split_80/dataset1_aug/val'\n",
    "test_dir = '/scratch/movi/dm_project/data/split_80/dataset1_aug/test'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.0001]\n",
    "epoch_counts = [25]\n",
    "NUM_CLASSES = 9\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        return cv2.medianBlur(image, 3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in median blur: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        image = image.astype(np.uint8)\n",
    "        return cv2.filter2D(image, -1, kernel)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in basic sharpen: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image_float = image.astype(float)\n",
    "        for i in range(3):\n",
    "            p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "            image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "        return image_float.astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in contrast stretch: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Custom CNN Model Definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.cpu())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "        logger.info(f\"Time for epoch {epoch + 1}: {epoch_time:.2f}s\")\n",
    "\n",
    "    logger.info(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "def test_and_evaluate(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate and log macro F1 score\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Create classification metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    metrics_df = np.array([precision, recall, f1]).T\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap=\"viridis\", xticklabels=[\"Precision\", \"Recall\", \"F1\"], yticklabels=class_names)\n",
    "    plt.title(\"Dataset 01 CustomCNN Classification Report\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.savefig('dataset_01_customcnn_classification_report.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Dataset 01 CustomCNN Confusion Matrix\")\n",
    "    plt.savefig('dataset_01_customcnn_confusion_matrix.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create ROC curves\n",
    "    all_labels_binarized = label_binarize(all_labels, classes=np.arange(NUM_CLASSES))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Dataset 01 CustomCNN ROC Curves for All Classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('dataset_01_customcnn_ROC_All_Classes.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Training and Evaluation Pipeline\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epoch_counts:\n",
    "            logger.info(f\"Training with Batch Size: {batch_size}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = CustomCNN()\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_and_validate(\n",
    "                model, train_loader, val_loader, criterion, optimizer, epochs\n",
    "            )\n",
    "\n",
    "            # Save accuracy and loss graphs\n",
    "            epochs_range = range(1, epochs + 1)\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Dataset 01 CustomCNN Accuracy (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_01_customcnn_accuracy_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Dataset 01 CustomCNN Loss (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_01_customcnn_loss_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            # Test and evaluate model\n",
    "            test_and_evaluate(model, test_loader, class_names=test_dataset.classes)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), 'dataset_01_customcnn_model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 23:52:28,348 - Training dataset size: 8679\n",
      "2024-11-21 23:52:28,349 - Validation dataset size: 1034\n",
      "2024-11-21 23:52:28,349 - Test dataset size: 1199\n",
      "2024-11-21 23:52:28,351 - Training with Batch Size: 128, Learning Rate: 0.0001, Epochs: 25\n",
      "2024-11-21 23:53:11,006 - Epoch 1/25 - Training: Loss = 1.8684, Accuracy = 0.4004\n",
      "2024-11-21 23:53:15,863 - Epoch 1/25 - Validation: Loss = 1.6312, Accuracy = 0.5377\n",
      "2024-11-21 23:53:15,864 - Time for epoch 1: 47.51s\n",
      "2024-11-21 23:53:54,535 - Epoch 2/25 - Training: Loss = 1.4408, Accuracy = 0.6501\n",
      "2024-11-21 23:53:58,960 - Epoch 2/25 - Validation: Loss = 1.3799, Accuracy = 0.6673\n",
      "2024-11-21 23:53:58,961 - Time for epoch 2: 43.10s\n",
      "2024-11-21 23:54:38,363 - Epoch 3/25 - Training: Loss = 1.2150, Accuracy = 0.7401\n",
      "2024-11-21 23:54:42,859 - Epoch 3/25 - Validation: Loss = 1.2163, Accuracy = 0.7244\n",
      "2024-11-21 23:54:42,860 - Time for epoch 3: 43.90s\n",
      "2024-11-21 23:55:23,159 - Epoch 4/25 - Training: Loss = 1.0510, Accuracy = 0.7946\n",
      "2024-11-21 23:55:27,529 - Epoch 4/25 - Validation: Loss = 1.0827, Accuracy = 0.7505\n",
      "2024-11-21 23:55:27,530 - Time for epoch 4: 44.67s\n",
      "2024-11-21 23:56:06,981 - Epoch 5/25 - Training: Loss = 0.9210, Accuracy = 0.8315\n",
      "2024-11-21 23:56:11,360 - Epoch 5/25 - Validation: Loss = 0.9866, Accuracy = 0.7872\n",
      "2024-11-21 23:56:11,362 - Time for epoch 5: 43.83s\n",
      "2024-11-21 23:56:49,712 - Epoch 6/25 - Training: Loss = 0.8113, Accuracy = 0.8587\n",
      "2024-11-21 23:56:54,126 - Epoch 6/25 - Validation: Loss = 0.8995, Accuracy = 0.8124\n",
      "2024-11-21 23:56:54,127 - Time for epoch 6: 42.76s\n",
      "2024-11-21 23:57:35,664 - Epoch 7/25 - Training: Loss = 0.7153, Accuracy = 0.8835\n",
      "2024-11-21 23:57:40,575 - Epoch 7/25 - Validation: Loss = 0.8256, Accuracy = 0.8337\n",
      "2024-11-21 23:57:40,575 - Time for epoch 7: 46.45s\n",
      "2024-11-21 23:58:20,074 - Epoch 8/25 - Training: Loss = 0.6340, Accuracy = 0.9046\n",
      "2024-11-21 23:58:24,519 - Epoch 8/25 - Validation: Loss = 0.7994, Accuracy = 0.8124\n",
      "2024-11-21 23:58:24,520 - Time for epoch 8: 43.94s\n",
      "2024-11-21 23:59:02,949 - Epoch 9/25 - Training: Loss = 0.5633, Accuracy = 0.9180\n",
      "2024-11-21 23:59:08,254 - Epoch 9/25 - Validation: Loss = 0.7358, Accuracy = 0.8395\n",
      "2024-11-21 23:59:08,254 - Time for epoch 9: 43.73s\n",
      "2024-11-21 23:59:48,345 - Epoch 10/25 - Training: Loss = 0.5011, Accuracy = 0.9342\n",
      "2024-11-21 23:59:53,445 - Epoch 10/25 - Validation: Loss = 0.6606, Accuracy = 0.8540\n",
      "2024-11-21 23:59:53,445 - Time for epoch 10: 45.19s\n",
      "2024-11-22 00:00:34,548 - Epoch 11/25 - Training: Loss = 0.4453, Accuracy = 0.9457\n",
      "2024-11-22 00:00:40,121 - Epoch 11/25 - Validation: Loss = 0.6141, Accuracy = 0.8636\n",
      "2024-11-22 00:00:40,122 - Time for epoch 11: 46.68s\n",
      "2024-11-22 00:01:21,297 - Epoch 12/25 - Training: Loss = 0.3969, Accuracy = 0.9535\n",
      "2024-11-22 00:01:26,255 - Epoch 12/25 - Validation: Loss = 0.6028, Accuracy = 0.8636\n",
      "2024-11-22 00:01:26,256 - Time for epoch 12: 46.13s\n",
      "2024-11-22 00:02:06,146 - Epoch 13/25 - Training: Loss = 0.3537, Accuracy = 0.9629\n",
      "2024-11-22 00:02:10,565 - Epoch 13/25 - Validation: Loss = 0.5736, Accuracy = 0.8569\n",
      "2024-11-22 00:02:10,566 - Time for epoch 13: 44.31s\n",
      "2024-11-22 00:02:52,653 - Epoch 14/25 - Training: Loss = 0.3174, Accuracy = 0.9703\n",
      "2024-11-22 00:02:57,599 - Epoch 14/25 - Validation: Loss = 0.5248, Accuracy = 0.8810\n",
      "2024-11-22 00:02:57,601 - Time for epoch 14: 47.03s\n",
      "2024-11-22 00:03:40,989 - Epoch 15/25 - Training: Loss = 0.2821, Accuracy = 0.9774\n",
      "2024-11-22 00:03:46,294 - Epoch 15/25 - Validation: Loss = 0.5046, Accuracy = 0.8762\n",
      "2024-11-22 00:03:46,295 - Time for epoch 15: 48.69s\n",
      "2024-11-22 00:04:31,606 - Epoch 16/25 - Training: Loss = 0.2530, Accuracy = 0.9803\n",
      "2024-11-22 00:04:36,509 - Epoch 16/25 - Validation: Loss = 0.4846, Accuracy = 0.8723\n",
      "2024-11-22 00:04:36,510 - Time for epoch 16: 50.21s\n",
      "2024-11-22 00:05:19,195 - Epoch 17/25 - Training: Loss = 0.2289, Accuracy = 0.9846\n",
      "2024-11-22 00:05:25,093 - Epoch 17/25 - Validation: Loss = 0.4588, Accuracy = 0.8926\n",
      "2024-11-22 00:05:25,094 - Time for epoch 17: 48.58s\n",
      "2024-11-22 00:06:10,928 - Epoch 18/25 - Training: Loss = 0.2053, Accuracy = 0.9862\n",
      "2024-11-22 00:06:16,097 - Epoch 18/25 - Validation: Loss = 0.4613, Accuracy = 0.8849\n",
      "2024-11-22 00:06:16,098 - Time for epoch 18: 51.00s\n",
      "2024-11-22 00:06:59,231 - Epoch 19/25 - Training: Loss = 0.1821, Accuracy = 0.9911\n",
      "2024-11-22 00:07:04,102 - Epoch 19/25 - Validation: Loss = 0.4200, Accuracy = 0.8936\n",
      "2024-11-22 00:07:04,103 - Time for epoch 19: 48.00s\n",
      "2024-11-22 00:07:47,110 - Epoch 20/25 - Training: Loss = 0.1673, Accuracy = 0.9920\n",
      "2024-11-22 00:07:51,986 - Epoch 20/25 - Validation: Loss = 0.4043, Accuracy = 0.8868\n",
      "2024-11-22 00:07:51,987 - Time for epoch 20: 47.88s\n",
      "2024-11-22 00:08:37,789 - Epoch 21/25 - Training: Loss = 0.1501, Accuracy = 0.9945\n",
      "2024-11-22 00:08:43,425 - Epoch 21/25 - Validation: Loss = 0.3954, Accuracy = 0.8985\n",
      "2024-11-22 00:08:43,426 - Time for epoch 21: 51.44s\n",
      "2024-11-22 00:09:28,466 - Epoch 22/25 - Training: Loss = 0.1332, Accuracy = 0.9962\n",
      "2024-11-22 00:09:33,301 - Epoch 22/25 - Validation: Loss = 0.3882, Accuracy = 0.9004\n",
      "2024-11-22 00:09:33,301 - Time for epoch 22: 49.87s\n",
      "2024-11-22 00:10:17,228 - Epoch 23/25 - Training: Loss = 0.1206, Accuracy = 0.9969\n",
      "2024-11-22 00:10:22,116 - Epoch 23/25 - Validation: Loss = 0.4110, Accuracy = 0.8946\n",
      "2024-11-22 00:10:22,117 - Time for epoch 23: 48.81s\n",
      "2024-11-22 00:11:03,615 - Epoch 24/25 - Training: Loss = 0.1077, Accuracy = 0.9978\n",
      "2024-11-22 00:11:08,186 - Epoch 24/25 - Validation: Loss = 0.3967, Accuracy = 0.8917\n",
      "2024-11-22 00:11:08,187 - Time for epoch 24: 46.07s\n",
      "2024-11-22 00:11:48,426 - Epoch 25/25 - Training: Loss = 0.0985, Accuracy = 0.9985\n",
      "2024-11-22 00:11:55,061 - Epoch 25/25 - Validation: Loss = 0.3904, Accuracy = 0.8820\n",
      "2024-11-22 00:11:55,062 - Time for epoch 25: 46.87s\n",
      "2024-11-22 00:11:55,062 - Total Training Time: 1166.67s\n",
      "2024-11-22 00:12:00,960 - Macro-Averaged F1 Score: 0.8657\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-21\n",
    "# Image classification pipeline using a custom simple CNN model and enhanced preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "train_dir = '/scratch/movi/dm_project/data/split_80/dataset2_aug/train'\n",
    "val_dir = '/scratch/movi/dm_project/data/split_80/dataset2_aug/val'\n",
    "test_dir = '/scratch/movi/dm_project/data/split_80/dataset2_aug/test'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.0001]\n",
    "epoch_counts = [25]\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        return cv2.medianBlur(image, 3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in median blur: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        image = image.astype(np.uint8)\n",
    "        return cv2.filter2D(image, -1, kernel)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in basic sharpen: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image_float = image.astype(float)\n",
    "        for i in range(3):\n",
    "            p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "            image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "        return image_float.astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in contrast stretch: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Custom CNN Model Definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.cpu())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "        logger.info(f\"Time for epoch {epoch + 1}: {epoch_time:.2f}s\")\n",
    "\n",
    "    logger.info(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "def test_and_evaluate(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate and log macro F1 score\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Create classification metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    metrics_df = np.array([precision, recall, f1]).T\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap=\"viridis\", xticklabels=[\"Precision\", \"Recall\", \"F1\"], yticklabels=class_names)\n",
    "    plt.title(\"Dataset 02 CustomCNN Classification Report\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.savefig('dataset_02_customcnn_classification_report.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Dataset 02 CustomCNN Confusion Matrix\")\n",
    "    plt.savefig('dataset_02_customcnn_confusion_matrix.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create ROC curves\n",
    "    all_labels_binarized = label_binarize(all_labels, classes=np.arange(NUM_CLASSES))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Dataset 02 CustomCNN ROC Curves for All Classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('dataset_02_customcnn_ROC_All_Classes.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Training and Evaluation Pipeline\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epoch_counts:\n",
    "            logger.info(f\"Training with Batch Size: {batch_size}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = CustomCNN()\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_and_validate(\n",
    "                model, train_loader, val_loader, criterion, optimizer, epochs\n",
    "            )\n",
    "\n",
    "            # Save accuracy and loss graphs\n",
    "            epochs_range = range(1, epochs + 1)\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Dataset 02 CustomCNN Accuracy (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_02_customcnn_accuracy_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Dataset 02 CustomCNN Loss (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_02_customcnn_loss_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            # Test and evaluate model\n",
    "            test_and_evaluate(model, test_loader, class_names=test_dataset.classes)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), 'dataset_02_customcnn_model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 23:27:17,375 - Training dataset size: 11594\n",
      "2024-11-21 23:27:17,376 - Validation dataset size: 1408\n",
      "2024-11-21 23:27:17,376 - Test dataset size: 1529\n",
      "2024-11-21 23:27:17,378 - Training with Batch Size: 128, Learning Rate: 5e-05, Epochs: 25\n",
      "2024-11-21 23:28:08,675 - Epoch 1/25 - Training: Loss = 1.7521, Accuracy = 0.3510\n",
      "2024-11-21 23:28:14,869 - Epoch 1/25 - Validation: Loss = 1.4719, Accuracy = 0.5504\n",
      "2024-11-21 23:28:14,869 - Time for epoch 1: 57.48s\n",
      "2024-11-21 23:29:05,866 - Epoch 2/25 - Training: Loss = 1.4121, Accuracy = 0.5955\n",
      "2024-11-21 23:29:11,976 - Epoch 2/25 - Validation: Loss = 1.3028, Accuracy = 0.6662\n",
      "2024-11-21 23:29:11,977 - Time for epoch 2: 57.11s\n",
      "2024-11-21 23:30:02,657 - Epoch 3/25 - Training: Loss = 1.2419, Accuracy = 0.6938\n",
      "2024-11-21 23:30:08,530 - Epoch 3/25 - Validation: Loss = 1.1611, Accuracy = 0.7031\n",
      "2024-11-21 23:30:08,532 - Time for epoch 3: 56.55s\n",
      "2024-11-21 23:31:01,357 - Epoch 4/25 - Training: Loss = 1.1001, Accuracy = 0.7550\n",
      "2024-11-21 23:31:07,215 - Epoch 4/25 - Validation: Loss = 1.0468, Accuracy = 0.7614\n",
      "2024-11-21 23:31:07,217 - Time for epoch 4: 58.68s\n",
      "2024-11-21 23:31:57,022 - Epoch 5/25 - Training: Loss = 0.9731, Accuracy = 0.8095\n",
      "2024-11-21 23:32:02,854 - Epoch 5/25 - Validation: Loss = 0.9555, Accuracy = 0.7933\n",
      "2024-11-21 23:32:02,856 - Time for epoch 5: 55.64s\n",
      "2024-11-21 23:32:54,031 - Epoch 6/25 - Training: Loss = 0.8595, Accuracy = 0.8498\n",
      "2024-11-21 23:33:00,349 - Epoch 6/25 - Validation: Loss = 0.8574, Accuracy = 0.8281\n",
      "2024-11-21 23:33:00,350 - Time for epoch 6: 57.49s\n",
      "2024-11-21 23:33:50,930 - Epoch 7/25 - Training: Loss = 0.7661, Accuracy = 0.8786\n",
      "2024-11-21 23:33:58,260 - Epoch 7/25 - Validation: Loss = 0.7729, Accuracy = 0.8594\n",
      "2024-11-21 23:33:58,261 - Time for epoch 7: 57.91s\n",
      "2024-11-21 23:34:50,621 - Epoch 8/25 - Training: Loss = 0.6812, Accuracy = 0.9070\n",
      "2024-11-21 23:34:57,049 - Epoch 8/25 - Validation: Loss = 0.7010, Accuracy = 0.8814\n",
      "2024-11-21 23:34:57,050 - Time for epoch 8: 58.79s\n",
      "2024-11-21 23:35:47,982 - Epoch 9/25 - Training: Loss = 0.6080, Accuracy = 0.9239\n",
      "2024-11-21 23:35:53,812 - Epoch 9/25 - Validation: Loss = 0.6366, Accuracy = 0.8892\n",
      "2024-11-21 23:35:53,814 - Time for epoch 9: 56.76s\n",
      "2024-11-21 23:36:45,091 - Epoch 10/25 - Training: Loss = 0.5426, Accuracy = 0.9366\n",
      "2024-11-21 23:36:51,093 - Epoch 10/25 - Validation: Loss = 0.5850, Accuracy = 0.9006\n",
      "2024-11-21 23:36:51,095 - Time for epoch 10: 57.28s\n",
      "2024-11-21 23:37:41,073 - Epoch 11/25 - Training: Loss = 0.4895, Accuracy = 0.9461\n",
      "2024-11-21 23:37:47,347 - Epoch 11/25 - Validation: Loss = 0.5464, Accuracy = 0.9134\n",
      "2024-11-21 23:37:47,348 - Time for epoch 11: 56.25s\n",
      "2024-11-21 23:38:41,012 - Epoch 12/25 - Training: Loss = 0.4370, Accuracy = 0.9564\n",
      "2024-11-21 23:38:48,007 - Epoch 12/25 - Validation: Loss = 0.4983, Accuracy = 0.9190\n",
      "2024-11-21 23:38:48,008 - Time for epoch 12: 60.66s\n",
      "2024-11-21 23:39:43,099 - Epoch 13/25 - Training: Loss = 0.3939, Accuracy = 0.9642\n",
      "2024-11-21 23:39:49,852 - Epoch 13/25 - Validation: Loss = 0.4582, Accuracy = 0.9325\n",
      "2024-11-21 23:39:49,853 - Time for epoch 13: 61.84s\n",
      "2024-11-21 23:40:43,431 - Epoch 14/25 - Training: Loss = 0.3574, Accuracy = 0.9700\n",
      "2024-11-21 23:40:49,388 - Epoch 14/25 - Validation: Loss = 0.4179, Accuracy = 0.9304\n",
      "2024-11-21 23:40:49,390 - Time for epoch 14: 59.53s\n",
      "2024-11-21 23:41:44,899 - Epoch 15/25 - Training: Loss = 0.3226, Accuracy = 0.9772\n",
      "2024-11-21 23:41:51,872 - Epoch 15/25 - Validation: Loss = 0.3934, Accuracy = 0.9474\n",
      "2024-11-21 23:41:51,874 - Time for epoch 15: 62.48s\n",
      "2024-11-21 23:42:48,294 - Epoch 16/25 - Training: Loss = 0.2966, Accuracy = 0.9777\n",
      "2024-11-21 23:42:54,945 - Epoch 16/25 - Validation: Loss = 0.3712, Accuracy = 0.9503\n",
      "2024-11-21 23:42:54,946 - Time for epoch 16: 63.07s\n",
      "2024-11-21 23:43:51,659 - Epoch 17/25 - Training: Loss = 0.2658, Accuracy = 0.9815\n",
      "2024-11-21 23:43:57,663 - Epoch 17/25 - Validation: Loss = 0.3433, Accuracy = 0.9545\n",
      "2024-11-21 23:43:57,664 - Time for epoch 17: 62.72s\n",
      "2024-11-21 23:44:48,391 - Epoch 18/25 - Training: Loss = 0.2437, Accuracy = 0.9846\n",
      "2024-11-21 23:44:54,336 - Epoch 18/25 - Validation: Loss = 0.3232, Accuracy = 0.9588\n",
      "2024-11-21 23:44:54,337 - Time for epoch 18: 56.67s\n",
      "2024-11-21 23:45:48,984 - Epoch 19/25 - Training: Loss = 0.2213, Accuracy = 0.9874\n",
      "2024-11-21 23:45:55,438 - Epoch 19/25 - Validation: Loss = 0.2969, Accuracy = 0.9595\n",
      "2024-11-21 23:45:55,439 - Time for epoch 19: 61.10s\n",
      "2024-11-21 23:46:47,590 - Epoch 20/25 - Training: Loss = 0.2049, Accuracy = 0.9891\n",
      "2024-11-21 23:46:53,619 - Epoch 20/25 - Validation: Loss = 0.2852, Accuracy = 0.9631\n",
      "2024-11-21 23:46:53,620 - Time for epoch 20: 58.18s\n",
      "2024-11-21 23:47:52,800 - Epoch 21/25 - Training: Loss = 0.1848, Accuracy = 0.9901\n",
      "2024-11-21 23:48:00,388 - Epoch 21/25 - Validation: Loss = 0.2726, Accuracy = 0.9538\n",
      "2024-11-21 23:48:00,389 - Time for epoch 21: 66.77s\n",
      "2024-11-21 23:48:59,129 - Epoch 22/25 - Training: Loss = 0.1721, Accuracy = 0.9913\n",
      "2024-11-21 23:49:05,967 - Epoch 22/25 - Validation: Loss = 0.2553, Accuracy = 0.9624\n",
      "2024-11-21 23:49:05,968 - Time for epoch 22: 65.58s\n",
      "2024-11-21 23:50:03,706 - Epoch 23/25 - Training: Loss = 0.1587, Accuracy = 0.9938\n",
      "2024-11-21 23:50:10,295 - Epoch 23/25 - Validation: Loss = 0.2367, Accuracy = 0.9631\n",
      "2024-11-21 23:50:10,296 - Time for epoch 23: 64.33s\n",
      "2024-11-21 23:51:06,660 - Epoch 24/25 - Training: Loss = 0.1434, Accuracy = 0.9945\n",
      "2024-11-21 23:51:15,191 - Epoch 24/25 - Validation: Loss = 0.2252, Accuracy = 0.9602\n",
      "2024-11-21 23:51:15,192 - Time for epoch 24: 64.90s\n",
      "2024-11-21 23:52:12,109 - Epoch 25/25 - Training: Loss = 0.1343, Accuracy = 0.9954\n",
      "2024-11-21 23:52:18,813 - Epoch 25/25 - Validation: Loss = 0.2252, Accuracy = 0.9645\n",
      "2024-11-21 23:52:18,814 - Time for epoch 25: 63.62s\n",
      "2024-11-21 23:52:18,814 - Total Training Time: 1501.39s\n",
      "2024-11-21 23:52:26,426 - Macro-Averaged F1 Score: 0.9581\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-21\n",
    "# Image classification pipeline using a custom simple CNN model and enhanced preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "train_dir = '/scratch/movi/dm_project/data/split_80/dataset3_aug/train'\n",
    "val_dir = '/scratch/movi/dm_project/data/split_80/dataset3_aug/val'\n",
    "test_dir = '/scratch/movi/dm_project/data/split_80/dataset3_aug/test'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.00005]\n",
    "epoch_counts = [25]\n",
    "NUM_CLASSES = 8\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        return cv2.medianBlur(image, 3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in median blur: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        image = image.astype(np.uint8)\n",
    "        return cv2.filter2D(image, -1, kernel)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in basic sharpen: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image_float = image.astype(float)\n",
    "        for i in range(3):\n",
    "            p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "            image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "        return image_float.astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in contrast stretch: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Custom CNN Model Definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.cpu())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "        logger.info(f\"Time for epoch {epoch + 1}: {epoch_time:.2f}s\")\n",
    "\n",
    "    logger.info(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "def test_and_evaluate(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate and log macro F1 score\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Create classification metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    metrics_df = np.array([precision, recall, f1]).T\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap=\"viridis\", xticklabels=[\"Precision\", \"Recall\", \"F1\"], yticklabels=class_names)\n",
    "    plt.title(\"Dataset 03 CustomCNN Classification Report\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.savefig('dataset_03_customcnn_classification_report.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Dataset 03 CustomCNN Confusion Matrix\")\n",
    "    plt.savefig('dataset_03_customcnn_confusion_matrix.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create ROC curves\n",
    "    all_labels_binarized = label_binarize(all_labels, classes=np.arange(NUM_CLASSES))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Dataset 03 CustomCNN ROC Curves for All Classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('dataset_03_customcnn_ROC_All_Classes.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Training and Evaluation Pipeline\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epoch_counts:\n",
    "            logger.info(f\"Training with Batch Size: {batch_size}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = CustomCNN()\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_and_validate(\n",
    "                model, train_loader, val_loader, criterion, optimizer, epochs\n",
    "            )\n",
    "\n",
    "            # Save accuracy and loss graphs\n",
    "            epochs_range = range(1, epochs + 1)\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Dataset 03 CustomCNN Accuracy (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_03_customcnn_accuracy_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Dataset 03 CustomCNN Loss (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_03_customcnn_loss_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            # Test and evaluate model\n",
    "            test_and_evaluate(model, test_loader, class_names=test_dataset.classes)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), 'dataset_03_customcnn_model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 22:40:46,632 - Training dataset size: 21802\n",
      "2024-11-21 22:40:46,633 - Validation dataset size: 2673\n",
      "2024-11-21 22:40:46,634 - Test dataset size: 2827\n",
      "2024-11-21 22:40:46,635 - Training with Batch Size: 128, Learning Rate: 0.0001, Epochs: 25\n",
      "2024-11-21 22:42:22,016 - Epoch 1/25 - Training: Loss = 1.6640, Accuracy = 0.5024\n",
      "2024-11-21 22:42:33,182 - Epoch 1/25 - Validation: Loss = 1.4455, Accuracy = 0.5799\n",
      "2024-11-21 22:42:33,184 - Time for epoch 1: 106.54s\n",
      "2024-11-21 22:44:07,185 - Epoch 2/25 - Training: Loss = 1.1707, Accuracy = 0.7143\n",
      "2024-11-21 22:44:18,378 - Epoch 2/25 - Validation: Loss = 1.1013, Accuracy = 0.7119\n",
      "2024-11-21 22:44:18,379 - Time for epoch 2: 105.19s\n",
      "2024-11-21 22:45:55,168 - Epoch 3/25 - Training: Loss = 0.8873, Accuracy = 0.8020\n",
      "2024-11-21 22:46:07,502 - Epoch 3/25 - Validation: Loss = 0.8855, Accuracy = 0.7752\n",
      "2024-11-21 22:46:07,504 - Time for epoch 3: 109.12s\n",
      "2024-11-21 22:47:50,677 - Epoch 4/25 - Training: Loss = 0.6923, Accuracy = 0.8509\n",
      "2024-11-21 22:48:03,123 - Epoch 4/25 - Validation: Loss = 0.7155, Accuracy = 0.8339\n",
      "2024-11-21 22:48:03,124 - Time for epoch 4: 115.62s\n",
      "2024-11-21 22:49:45,780 - Epoch 5/25 - Training: Loss = 0.5585, Accuracy = 0.8813\n",
      "2024-11-21 22:49:57,928 - Epoch 5/25 - Validation: Loss = 0.6064, Accuracy = 0.8421\n",
      "2024-11-21 22:49:57,929 - Time for epoch 5: 114.80s\n",
      "2024-11-21 22:51:32,826 - Epoch 6/25 - Training: Loss = 0.4602, Accuracy = 0.9026\n",
      "2024-11-21 22:51:44,377 - Epoch 6/25 - Validation: Loss = 0.5147, Accuracy = 0.8709\n",
      "2024-11-21 22:51:44,378 - Time for epoch 6: 106.45s\n",
      "2024-11-21 22:53:18,525 - Epoch 7/25 - Training: Loss = 0.3817, Accuracy = 0.9225\n",
      "2024-11-21 22:53:29,990 - Epoch 7/25 - Validation: Loss = 0.4602, Accuracy = 0.8840\n",
      "2024-11-21 22:53:29,992 - Time for epoch 7: 105.61s\n",
      "2024-11-21 22:55:05,663 - Epoch 8/25 - Training: Loss = 0.3260, Accuracy = 0.9327\n",
      "2024-11-21 22:55:17,820 - Epoch 8/25 - Validation: Loss = 0.4013, Accuracy = 0.9012\n",
      "2024-11-21 22:55:17,820 - Time for epoch 8: 107.83s\n",
      "2024-11-21 22:56:51,640 - Epoch 9/25 - Training: Loss = 0.2764, Accuracy = 0.9472\n",
      "2024-11-21 22:57:02,682 - Epoch 9/25 - Validation: Loss = 0.3768, Accuracy = 0.8979\n",
      "2024-11-21 22:57:02,683 - Time for epoch 9: 104.86s\n",
      "2024-11-21 22:58:36,365 - Epoch 10/25 - Training: Loss = 0.2340, Accuracy = 0.9590\n",
      "2024-11-21 22:58:47,602 - Epoch 10/25 - Validation: Loss = 0.3405, Accuracy = 0.9151\n",
      "2024-11-21 22:58:47,602 - Time for epoch 10: 104.92s\n",
      "2024-11-21 23:00:24,243 - Epoch 11/25 - Training: Loss = 0.2032, Accuracy = 0.9637\n",
      "2024-11-21 23:00:36,053 - Epoch 11/25 - Validation: Loss = 0.3252, Accuracy = 0.9091\n",
      "2024-11-21 23:00:36,054 - Time for epoch 11: 108.45s\n",
      "2024-11-21 23:02:14,667 - Epoch 12/25 - Training: Loss = 0.1734, Accuracy = 0.9725\n",
      "2024-11-21 23:02:26,951 - Epoch 12/25 - Validation: Loss = 0.2959, Accuracy = 0.9218\n",
      "2024-11-21 23:02:26,952 - Time for epoch 12: 110.90s\n",
      "2024-11-21 23:04:03,420 - Epoch 13/25 - Training: Loss = 0.1526, Accuracy = 0.9761\n",
      "2024-11-21 23:04:14,759 - Epoch 13/25 - Validation: Loss = 0.2688, Accuracy = 0.9315\n",
      "2024-11-21 23:04:14,760 - Time for epoch 13: 107.81s\n",
      "2024-11-21 23:05:48,413 - Epoch 14/25 - Training: Loss = 0.1280, Accuracy = 0.9822\n",
      "2024-11-21 23:06:00,106 - Epoch 14/25 - Validation: Loss = 0.2713, Accuracy = 0.9214\n",
      "2024-11-21 23:06:00,107 - Time for epoch 14: 105.35s\n",
      "2024-11-21 23:07:42,343 - Epoch 15/25 - Training: Loss = 0.1136, Accuracy = 0.9850\n",
      "2024-11-21 23:07:54,711 - Epoch 15/25 - Validation: Loss = 0.2614, Accuracy = 0.9233\n",
      "2024-11-21 23:07:54,712 - Time for epoch 15: 114.60s\n",
      "2024-11-21 23:09:38,429 - Epoch 16/25 - Training: Loss = 0.0999, Accuracy = 0.9873\n",
      "2024-11-21 23:09:50,831 - Epoch 16/25 - Validation: Loss = 0.2268, Accuracy = 0.9413\n",
      "2024-11-21 23:09:50,832 - Time for epoch 16: 116.12s\n",
      "2024-11-21 23:11:33,554 - Epoch 17/25 - Training: Loss = 0.0867, Accuracy = 0.9902\n",
      "2024-11-21 23:11:45,883 - Epoch 17/25 - Validation: Loss = 0.2427, Accuracy = 0.9308\n",
      "2024-11-21 23:11:45,884 - Time for epoch 17: 115.05s\n",
      "2024-11-21 23:13:26,773 - Epoch 18/25 - Training: Loss = 0.0779, Accuracy = 0.9917\n",
      "2024-11-21 23:13:37,958 - Epoch 18/25 - Validation: Loss = 0.2296, Accuracy = 0.9300\n",
      "2024-11-21 23:13:37,960 - Time for epoch 18: 112.07s\n",
      "2024-11-21 23:15:13,603 - Epoch 19/25 - Training: Loss = 0.0677, Accuracy = 0.9935\n",
      "2024-11-21 23:15:27,023 - Epoch 19/25 - Validation: Loss = 0.2230, Accuracy = 0.9315\n",
      "2024-11-21 23:15:27,024 - Time for epoch 19: 109.06s\n",
      "2024-11-21 23:17:08,332 - Epoch 20/25 - Training: Loss = 0.0572, Accuracy = 0.9956\n",
      "2024-11-21 23:17:49,853 - Epoch 20/25 - Validation: Loss = 0.2225, Accuracy = 0.9353\n",
      "2024-11-21 23:17:49,855 - Time for epoch 20: 142.83s\n",
      "2024-11-21 23:19:26,567 - Epoch 21/25 - Training: Loss = 0.0534, Accuracy = 0.9961\n",
      "2024-11-21 23:19:38,782 - Epoch 21/25 - Validation: Loss = 0.2016, Accuracy = 0.9435\n",
      "2024-11-21 23:19:38,784 - Time for epoch 21: 108.93s\n",
      "2024-11-21 23:21:16,513 - Epoch 22/25 - Training: Loss = 0.0462, Accuracy = 0.9971\n",
      "2024-11-21 23:21:28,433 - Epoch 22/25 - Validation: Loss = 0.2168, Accuracy = 0.9319\n",
      "2024-11-21 23:21:28,434 - Time for epoch 22: 109.65s\n",
      "2024-11-21 23:23:05,058 - Epoch 23/25 - Training: Loss = 0.0416, Accuracy = 0.9972\n",
      "2024-11-21 23:23:17,706 - Epoch 23/25 - Validation: Loss = 0.2042, Accuracy = 0.9353\n",
      "2024-11-21 23:23:17,708 - Time for epoch 23: 109.27s\n",
      "2024-11-21 23:25:01,598 - Epoch 24/25 - Training: Loss = 0.0364, Accuracy = 0.9981\n",
      "2024-11-21 23:25:14,457 - Epoch 24/25 - Validation: Loss = 0.2023, Accuracy = 0.9383\n",
      "2024-11-21 23:25:14,458 - Time for epoch 24: 116.75s\n",
      "2024-11-21 23:26:50,610 - Epoch 25/25 - Training: Loss = 0.0319, Accuracy = 0.9991\n",
      "2024-11-21 23:27:02,129 - Epoch 25/25 - Validation: Loss = 0.1989, Accuracy = 0.9394\n",
      "2024-11-21 23:27:02,131 - Time for epoch 25: 107.67s\n",
      "2024-11-21 23:27:02,131 - Total Training Time: 2775.45s\n",
      "2024-11-21 23:27:15,347 - Macro-Averaged F1 Score: 0.9342\n"
     ]
    }
   ],
   "source": [
    "# Written by Ovi, 2024-11-21\n",
    "# Image classification pipeline using a custom simple CNN model and enhanced preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_fscore_support, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "train_dir = '/scratch/movi/dm_project/data/split_80/dataset4_aug/train'\n",
    "val_dir = '/scratch/movi/dm_project/data/split_80/dataset4_aug/val'\n",
    "test_dir = '/scratch/movi/dm_project/data/split_80/dataset4_aug/test'\n",
    "\n",
    "# Hyperparameters\n",
    "batch_sizes = [128]\n",
    "learning_rates = [0.0001]\n",
    "epoch_counts = [25]\n",
    "NUM_CLASSES = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Preprocessing Functions\n",
    "def apply_median_blur(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image = image.astype(np.uint8)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        return cv2.medianBlur(image, 3)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in median blur: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_basic_sharpen(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        image = image.astype(np.uint8)\n",
    "        return cv2.filter2D(image, -1, kernel)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in basic sharpen: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "def apply_contrast_stretch(image: np.ndarray) -> np.ndarray:\n",
    "    try:\n",
    "        image_float = image.astype(float)\n",
    "        for i in range(3):\n",
    "            p2, p98 = np.percentile(image_float[:, :, i], (2, 98))\n",
    "            image_float[:, :, i] = np.clip((image_float[:, :, i] - p2) / (p98 - p2) * 255, 0, 255)\n",
    "        return image_float.astype(np.uint8)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in contrast stretch: {str(e)}\")\n",
    "        return image\n",
    "\n",
    "class CLAHE:\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(img)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        img = cv2.merge((cl, a, b))\n",
    "        return transforms.functional.to_pil_image(cv2.cvtColor(img, cv2.COLOR_LAB2RGB))\n",
    "\n",
    "# Data Transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_median_blur(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_basic_sharpen(np.array(x)))),\n",
    "        transforms.Lambda(lambda x: Image.fromarray(apply_contrast_stretch(np.array(x)))),\n",
    "        CLAHE(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms['val'])\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=data_transforms['test'])\n",
    "\n",
    "logger.info(f\"Training dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Custom CNN Model Definition\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    train_acc_history, val_acc_history = [], []\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "    total_training_time = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = train_correct.double() / len(train_loader.dataset)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc.cpu())\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.cpu())\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_time\n",
    "\n",
    "        logger.info(f\"Epoch {epoch + 1}/{epochs} - Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "        logger.info(f\"Time for epoch {epoch + 1}: {epoch_time:.2f}s\")\n",
    "\n",
    "    logger.info(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n",
    "    return train_acc_history, val_acc_history, train_loss_history, val_loss_history\n",
    "\n",
    "def test_and_evaluate(model, test_loader, class_names):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Calculate and log macro F1 score\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    logger.info(f\"Macro-Averaged F1 Score: {macro_f1:.4f}\")\n",
    "\n",
    "    # Create classification metrics heatmap\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None, labels=np.unique(all_labels))\n",
    "    metrics_df = np.array([precision, recall, f1]).T\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(metrics_df, annot=True, cmap=\"viridis\", xticklabels=[\"Precision\", \"Recall\", \"F1\"], yticklabels=class_names)\n",
    "    plt.title(\"Combined Dataset CustomCNN Classification Report\")\n",
    "    plt.xlabel(\"Metric\")\n",
    "    plt.ylabel(\"Class\")\n",
    "    plt.savefig('dataset_04_customcnn_classification_report.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Combined Dataset CustomCNN Confusion Matrix\")\n",
    "    plt.savefig('dataset_04_customcnn_confusion_matrix.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "    # Create ROC curves\n",
    "    all_labels_binarized = label_binarize(all_labels, classes=np.arange(NUM_CLASSES))\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown', 'pink', 'gray'])\n",
    "    for i, color in zip(range(NUM_CLASSES), colors):\n",
    "        fpr, tpr, _ = roc_curve(all_labels_binarized[:, i], all_probs[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, color=color, lw=2, label=f'Class {class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Combined Dataset CustomCNN ROC Curves for All Classes')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('dataset_04_customcnn_ROC_All_Classes.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()\n",
    "\n",
    "# Training and Evaluation Pipeline\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epoch_counts:\n",
    "            logger.info(f\"Training with Batch Size: {batch_size}, Learning Rate: {lr}, Epochs: {epochs}\")\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = CustomCNN()\n",
    "            model = model.to(DEVICE)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "            train_acc_history, val_acc_history, train_loss_history, val_loss_history = train_and_validate(\n",
    "                model, train_loader, val_loader, criterion, optimizer, epochs\n",
    "            )\n",
    "\n",
    "            # Save accuracy and loss graphs\n",
    "            epochs_range = range(1, epochs + 1)\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_acc_history, label='Training Accuracy')\n",
    "            plt.plot(epochs_range, val_acc_history, label='Validation Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title(f'Combined Dataset CustomCNN Accuracy (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_04_customcnn_accuracy_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure()\n",
    "            plt.plot(epochs_range, train_loss_history, label='Training Loss')\n",
    "            plt.plot(epochs_range, val_loss_history, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title(f'Combined Dataset CustomCNN Loss (Batch Size {batch_size}, LR {lr}, Epochs {epochs})')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'dataset_04_customcnn_loss_batch_{batch_size}_lr_{lr}_epochs_{epochs}.png', dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "            plt.close()\n",
    "\n",
    "            # Test and evaluate model\n",
    "            test_and_evaluate(model, test_loader, class_names=test_dataset.classes)\n",
    "\n",
    "            # Save the trained model\n",
    "            torch.save(model.state_dict(), 'dataset_04_customcnn_model_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 286794\n",
      "Trainable parameters: 286794\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' is your defined PyTorch model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total parameters: {total_params}')\n",
    "print(f'Trainable parameters: {trainable_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
